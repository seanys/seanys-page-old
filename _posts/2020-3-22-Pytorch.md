---
layout: post
title: "Pytorch学习笔记"
subtitle: "Sean Yang's learning record of Pytorch"
author: "sean"
header-img: "img/post-bg-ai.jpeg"
tags:
  - Deep Learning

---

## 基础模型

```python
import torch
import torch.utils.data as Data

# 批训练的数据个数
BATCH_SIZE = 5 

# torch生成xy，可以回归为线性函数
x = torch.linspace(1, 10, 10)
y = torch.linspace(10, 1, 10)  

# 先转换成 torch 能识别的 Dataset
torch_dataset = Data.TensorDataset(data_tensor=x, target_tensor=y)

# 把 dataset 放入 DataLoader
loader = Data.DataLoader(
    dataset=torch_dataset,      # torch TensorDataset format
    batch_size=BATCH_SIZE,      # mini batch size
    shuffle=True,               # 要不要打乱数据 (打乱比较好)
    num_workers=2,              # 多线程来读数据
)

for epoch in range(3):   # 训练所有数据 3 次
    for step, (batch_x, batch_y) in enumerate(loader):  # 每一步 loader 释放一小批数据用来学习
        # 假设这里就是你训练的地方...

        # 打出来一些数据
        print('Epoch: ', epoch, '| Step: ', step, '| batch x: ',
              batch_x.numpy(), '| batch y: ', batch_y.numpy())
```



## 网络模型

### 激活函数



### 自动求导



## 典型网络

### RNN

对输入序列中每个元素，`RNN`每层的计算公式为 $$ h_t=tanh(w_{ih} *x_t+b_{ih}+w_{hh}* h_{t-1}+b_{hh}) $$ $h_t$是时刻$t$的隐状态。 $x_t$是上一层时刻$t$的隐状态，或者是第一层在时刻$t$的输入。如果`nonlinearity='relu'`,那么将使用`relu`代替`tanh`作为激活函数。

### LSTM

对输入序列的每个元素，`LSTM`的每层都会执行以下计算： 
$$
\begin{aligned} i_t &= sigmoid(W_{ii}x_t+b_{ii}+W_{hi}h_{t-1}+b_{hi}) \ f_t \\ &= 
sigmoid(W_{if}x_t+b_{if}+W_{hf}h_{t-1}+b_{hf}) \ o_t \\ &= sigmoid(W_{io}x_t+b_{io}+W_{ho}h_{t-1}+b_{ho})\ g_t \\ &= tanh(W_{ig}x_t+b_{ig}+W_{hg}h_{t-1}+b_{hg})\ c_t \\ &= f_t*c_{t-1}+i_t*g_t\ h_t \\ &= o_t*tanh(c_t) \end{aligned}
$$
$h_t$是时刻$t$的隐状态,$c_t$是时刻$t$的细胞状态，$x_t$是上一层的在时刻$t$的隐状态或者是第一层在时刻$t$的输入。$i_t, f_t, g_t, o_t$ 分别代表 输入门，遗忘门，细胞和输出门。

参数说明:

- input_size – 输入的特征维度
- hidden_size – 隐状态的特征维度
- num_layers – 层数（和时序展开要区分开）
- bias – 如果为`False`，那么`LSTM`将不会使用$b_{ih},b_{hh}$，默认为`True`。
- batch_first – 如果为`True`，那么输入和输出`Tensor`的形状为`(batch, seq, feature)`
- dropout – 如果非零的话，将会在`RNN`的输出上加个`dropout`，最后一层除外。
- bidirectional – 如果为`True`，将会变成一个双向`RNN`，默认为`False`。

`LSTM`输入: input, (h_0, c_0)

- input (seq_len, batch, input_size): 包含输入序列特征的`Tensor`。也可以是`packed variable` ，详见 [pack_padded_sequence](#torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False[source])
- h_0 (num_layers * num_directions, batch, hidden_size):保存着`batch`中每个元素的初始化隐状态的`Tensor`
- c_0 (num_layers * num_directions, batch, hidden_size): 保存着`batch`中每个元素的初始化细胞状态的`Tensor`

`LSTM`输出 output, (h_n, c_n)

- output (seq_len, batch, hidden_size * num_directions): 保存`RNN`最后一层的输出的`Tensor`。 如果输入是`torch.nn.utils.rnn.PackedSequence`，那么输出也是`torch.nn.utils.rnn.PackedSequence`。
- h_n (num_layers * num_directions, batch, hidden_size): `Tensor`，保存着`RNN`最后一个时间步的隐状态。
- c_n (num_layers * num_directions, batch, hidden_size): `Tensor`，保存着`RNN`最后一个时间步的细胞状态。

`LSTM`模型参数:

- weight_ih_l[k] – 第`k`层可学习的`input-hidden`权重($W_{ii}|W_{if}|W_{ig}|W_{io}$)，形状为`(input_size x 4*hidden_size)`
- weight_hh_l[k] – 第`k`层可学习的`hidden-hidden`权重($W_{hi}|W_{hf}|W_{hg}|W_{ho}$)，形状为`(hidden_size x 4*hidden_size)`。
- bias_ih_l[k] – 第`k`层可学习的`input-hidden`偏置($b_{ii}|b_{if}|b_{ig}|b_{io}$)，形状为`( 4*hidden_size)`
- bias_hh_l[k] – 第`k`层可学习的`hidden-hidden`偏置($b_{hi}|b_{hf}|b_{hg}|b_{ho}$)，形状为`( 4*hidden_size)`。 示例:

```
lstm = nn.LSTM(10, 20, 2)
input = Variable(torch.randn(5, 3, 10))
h0 = Variable(torch.randn(2, 3, 20))
c0 = Variable(torch.randn(2, 3, 20))
output, hn = lstm(input, (h0, c0))
```

### Dropout layers

```
class torch.nn.Dropout(p=0.5, inplace=False)
```

随机将输入张量中部分元素设置为0。对于每次前向调用，被置0的元素都是随机的。

**参数：**

- **p** - 将元素置0的概率。默认值：0.5
- **in-place** - 若设置为True，会在原地执行操作。默认值：False

**形状：**

- **输入：** 任意。输入可以为任意形状。
- **输出：** 相同。输出和输入形状相同。

**例子：**

```
>>> m = nn.Dropout(p=0.2)
>>> input = autograd.Variable(torch.randn(20, 16))
>>> output = m(input)
class torch.nn.Dropout2d(p=0.5, inplace=False)
```

随机将输入张量中整个通道设置为0。对于每次前向调用，被置0的通道都是随机的。

*通常输入来自Conv2d模块。*

像在论文[Efficient Object Localization Using Convolutional Networks](https://arxiv.org/abs/1411.4280)，如果特征图中相邻像素是强相关的（在前几层卷积层很常见），那么iid dropout不会归一化激活，而只会降低学习率。

在这种情形，`nn.Dropout2d()`可以提高特征图之间的独立程度，所以应该使用它。

**参数：**

- **p**(*[float](https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/), optional*) - 将元素置0的概率。
- **in-place**(*[bool,](https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/) optional*) - 若设置为True，会在原地执行操作。

**形状：**

- **输入：** (N,C,H,W)
- **输出：** (N,C,H,W)（与输入形状相同）

**例子：**

```
>>> m = nn.Dropout2d(p=0.2)
>>> input = autograd.Variable(torch.randn(20, 16, 32, 32))
>>> output = m(input)
class torch.nn.Dropout3d(p=0.5, inplace=False)
```

随机将输入张量中整个通道设置为0。对于每次前向调用，被置0的通道都是随机的。

*通常输入来自Conv3d模块。*

像在论文[Efficient Object Localization Using Convolutional Networks](https://arxiv.org/abs/1411.4280)，如果特征图中相邻像素是强相关的（在前几层卷积层很常见），那么iid dropout不会归一化激活，而只会降低学习率。

在这种情形，`nn.Dropout3d()`可以提高特征图之间的独立程度，所以应该使用它。

**参数：**

- **p**(*[float](https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/), optional*) - 将元素置0的概率。
- **in-place**(*[bool,](https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/) optional*) - 若设置为True，会在原地执行操作。

**形状：**

- **输入：** N,C,D,H,W)
- **输出：** (N,C,D,H,W)（与输入形状相同）

**例子：**

```python
m = nn.Dropout3d(p=0.2)
input = autograd.Variable(torch.randn(20, 16, 4, 32, 32))
output = m(input)
```



## 损失函数与优化

### 损失函数

#### 使用方式

```python
criterion = nn.CrossEntropyLoss() #构造函数有自己的参数
loss = criterion(outputs, labels) #调用标准时也有参数
```

如果传入 `size_average=False` 就不会平均，output和label的形状不限，会自动处理

#### L1Loss

定义：差的绝对值的平均值的标准

计算：$loss(x,y)=\frac{1}{n}\sum|output_i−label_i|$

#### MSELoss

定义：均方误差标准

计算：$loss(x,y)=\frac{1}{n}∑(output_i−label_i)^2$

交叉熵的计算公式为:

#### CrossEntropyLoss

**计算公式**：$𝑐𝑟𝑜𝑠𝑠E𝑛𝑡𝑟𝑜𝑝𝑦=−∑_{𝑘=1}^N(𝑝_𝑘∗log_𝑞𝑘)=-log\,q_m$

**原理**：其中$𝑝$表示真实值，在这个公式中是one-hot形式；$𝑞$是预测值，在这里假设已经是经过softmax后的结果了。

**简化版**：$𝑐𝑟𝑜𝑠𝑠E𝑛𝑡𝑟𝑜𝑝𝑦=−∑𝑘=1𝑁(𝑝𝑘∗log𝑞𝑘)=−𝑙𝑜𝑔𝑞𝑚cross_entropy=−∑k=1N(pk∗log⁡qk)=−logqm$

仔细看看，是不是就是等同于**log_softmax**和**nll_loss**两个步骤。

所以Pytorch中的**F.cross_entropy**会自动调用上面介绍的**log_softmax**和**nll_loss**来计算交叉熵,其计算方式如下:



loss(𝑥,class)=−log(exp(𝑥[class])∑𝑗exp(𝑥[𝑗]))

### 优化器

```python
optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9) 
optimizer = optim.Adam([var1, var2], lr = 0.0001)
```

优化器类别：SGD、Adam、RMSprop、Adamax、ASGD、LBFGS、Rprop

