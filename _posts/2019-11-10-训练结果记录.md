---
layout: post
title: "训练结果记录"
subtitle: '算法和实验结果的记录'
author: "sean"
header-img: "img/post-bg-ai.jpeg"
tags:
  - Bin packing
  - Machine Learnings

---

## 12.2 收尾处理

### 归一化的方法

#### 1）最大最小标准化（Min-Max Normalization）

本归一化方法又称为离差标准化，本归一化方法比较适用在数值比较集中的情况，使结果值映射到[0 ，1]之间，转换函数如下：

![为什么一些机器学习模型需要对数据进行归一化？](http://static.open-open.com/lib/uploadImg/20150422/20150422180153_979.png)

缺陷：如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。实际使用中可以用经验常量来替代max和min。

应用场景：在不涉及距离度量、协方差计算、数据不符合正太分布的时候，可以使用第一种方法或其他归一化方法（不包括Z-score方法）。比如图像处理中，将RGB图像转换为灰度图像后将其值限定在[0 255]的范围

#### 2）Z-score标准化方法

数据处理后符合标准正态分布，即均值为0，标准差为1，其转化函数为：

![为什么一些机器学习模型需要对数据进行归一化？](http://static.open-open.com/lib/uploadImg/20150422/20150422180153_825.png)

其中μ为所有样本数据的均值，σ为所有样本数据的标准差。

本方法要求原始数据的分布可以近似为高斯分布，否则归一化的效果会变得很糟糕；

应用场景：在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，Z-score standardization表现更好。

### [调参数参考 ](https://zhuanlan.zhihu.com/p/24720954)

#### 调参参考

- learning rate: 1 0.1 0.01 0.001, 一般从1开始尝试。很少见learning rate大于10的。学习率一般要随着训练进行衰减。衰减系数一般是0.5。 衰减时机，可以是验证集准确率不再上升时，或固定训练多少个周期以后。 
  不过更建议使用自适应梯度的办法，例如adam,adadelta,rmsprop等，这些一般使用相关论文提供的默认值即可，可以避免再费劲调节学习率。对RNN来说，有个经验，如果RNN要处理的序列比较长，或者RNN层数比较多，那么learning rate一般小一些比较好，否则有可能出现结果不收敛，甚至Nan等问题。
- 网络层数： 先从1层开始。
- 每层结点数： 16 32 128，超过1000的情况比较少见。超过1W的从来没有见过。
- batch size: 128上下开始。batch size值增加，的确能提高训练速度。但是有可能收敛结果变差。如果显存大小允许，可以考虑从一个比较大的值开始尝试。因为batch size太大，一般不会对结果有太大的影响，而batch size太小的话，结果有可能很差。
- clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值，就算一个衰减系系数,让value的值等于阈值: 5,10,15
- dropout： 0.5
- L2正则：1.0，超过10的很少见。
- 词向量embedding大小：128，256
- 正负样本比例： 这个是非常忽视，但是在很多分类问题上，又非常重要的参数。很多人往往习惯使用训练数据中默认的正负类别比例，当训练数据非常不平衡的时候，模型很有可能会偏向数目较大的类别，从而影响最终训练结果。除了尝试训练数据默认的正负类别比例之外，建议对数目较小的样本做过采样，例如进行复制。提高他们的比例，看看效果如何，这个对多分类问题同样适用。 
  在使用mini-batch方法进行训练的时候，尽量让一个batch内，各类别的比例平衡，这个在图像识别等多分类任务上非常重要。

#### 自动调参

人工一直盯着实验，毕竟太累。自动调参当前也有不少研究。下面介绍几种比较实用的办法：

- Gird Search. 这个是最常见的。具体说，就是每种参数确定好几个要尝试的值，然后像一个网格一样，把所有参数值的组合遍历一下。优点是实现简单暴力，如果能全部遍历的话，结果比较可靠。缺点是太费时间了，特别像神经网络，一般尝试不了太多的参数组合。
- Random Search。Bengio在[Random Search for Hyper-Parameter Optimization](https://link.zhihu.com/?target=http%3A//www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)中指出，Random Search比Gird Search更有效。实际操作的时候，一般也是先用Gird Search的方法，得到所有候选参数，然后每次从中随机选择进行训练。
- Bayesian Optimization. 贝叶斯优化，考虑到了不同参数对应的实验结果值，因此更节省时间。和网络搜索相比简直就是老牛和跑车的区别。具体原理可以参考这个论文：Practical Bayesian Optimization of Machine Learning Algorithms，这里同时推荐两个实现了贝叶斯调参的Python库，可以上手即用：
  - [jaberg/hyperopt](https://link.zhihu.com/?target=https%3A//github.com/jaberg/hyperopt), 比较简单。
  - [fmfn/BayesianOptimization](https://link.zhihu.com/?target=https%3A//github.com/fmfn/BayesianOptimization)， 比较复杂，支持并行调参。





## 12.1 补充记录

### 优化器选择

https://www.cnblogs.com/guoyaohua/p/8542554.html

### 训练集与测试集

https://zhuanlan.zhihu.com/p/35394638

### 隐藏层个数/深度学习

深度学习的“深度”有什么意义？ - 纳米酱的回答 - 知乎 https://www.zhihu.com/question/25456959/answer/66199206

剃刀原理：合理即可

256-128-64-32多层

<img src="/Users/sean/Library/Application Support/typora-user-images/image-20191202082804262.png" alt="image-20191202082804262" style="zoom:50%;" />

256-128-64层-选择这个

<img src="/Users/sean/Library/Application Support/typora-user-images/image-20191202083425023.png" alt="image-20191202083425023" style="zoom:50%;" />

256-128层-选择这个

<img src="/Users/sean/Library/Application Support/typora-user-images/image-20191202083524280.png" alt="image-20191202083524280" style="zoom:50%;" />

## 11.27 训练记录

### 数据来源

参考jakjob选择了两个形状，不规则三角形和缺了一个口的长方形，随机生成对应的形状

#### 缺口长方形

```python
						width=random.randint(100,500)
            height=random.randint(100,500)
            poly=[[500,500],[500+width,500],[500+width,500+height],[500,500+height]] #生成初始形状
            extend_poly=poly+poly
            # 生成随机位置和获得点
            choose_index=random.randint(3,6)
            pt=extend_poly[choose_index]
            pre_pt=extend_poly[choose_index-1]
            next_pt=extend_poly[choose_index+1]
            # 获得中间点
            lambda1=random.uniform(0.2,0.8)
            lambda2=random.uniform(0.2,0.8)
            mid_pre=self.getMid(pre_pt,pt,lambda1)
            mid_next=self.getMid(pt,next_pt,lambda2)
            # 获得新的形状
            for i in range(3,7):
                if i==choose_index:
                    new_poly.append(mid_pre)
                    if mid_pre[0]==extend_poly[choose_index-1][0]:
                        new_poly.append([mid_next[0],mid_pre[1]])
                    else:
                        new_poly.append([mid_pre[0],mid_next[1]])
                    new_poly.append(mid_next)
                else:
                    new_poly.append(extend_poly[i])
            return new_poly
```

#### 三角形

```python
						pt1=[250,250]
            pt2=[]
            pt3=[]
            if random.randint(0,1)==0:
                pt2=[250+random.randint(100,250),250]
                pt3=[250+random.randint(25,pt2[0]+20),250+random.randint(50,250)]
            else:
                pt2=[250+random.randint(25,250),250-random.randint(50,250)]
                pt3=[250+random.randint(pt2[0]-270,250),250]
            return [pt1,pt2,pt3]
```

#### 组合模式

一般情况下是在空缺位置、底部、左侧和右侧、上侧和下侧平的地方

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g9d1t0iuelj30u00xfjtf.jpg" alt="image-20191128001024652" style="zoom:33%;" />

#### 采用该数据原因

euro上的数据本质上属于不同的模型，尽管上次缩放到了类似的相同尺寸，但是还是有两个很大的问题

- 形状非常有限，也就是输入有限，造成学习困难
- 形状大小仍然尺寸不一，造成了无法处理很多的形状，因为数据集不够

所以这次采用了相对规范化和尺寸有一定限制（但是仍然是随机生成）的数据集，这样就肯定可以学习这样两个样本的组合模式，后续如果加入更多的模型，也是可以学习的

### 训练结果

#### 训练网络

数据标准化：y是centroid的方向向量进行标准化(0-1)，x对centroid outside的数据进行了清理，暂时不考虑

损失函数：交差熵损失函数

#### 排样结果

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g9d37jjgm0j30sc110aap.jpg" alt="image-20191128005857304" style="zoom:33%;" />

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g9d37ts8x3j30r00voab8.jpg" alt="image-20191128005913649" style="zoom:33%;" />

#### 统计结果

4000个样本训练，2000个样本检验，1000 epoch，batch size=256，简单的反向传播网络

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g9d34isq8bj30zk0qo0uc.jpg" alt="accu_pos_pos_1000_layer_256_128_32_batch_256" style="zoom:50%;" />

1000个样本训练，剩下5000个样本检验，100 epoch，batch size=256，简单的反向传播网络

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g9d34ocyhhj30zk0qojtp.jpg" alt="acc_sample_1000_pos_pos_epoch_100_layer_256_128_32_batch_256" style="zoom:50%;" />

### 结果分析

1. **类似于线性回归**：向量表示了形状的特征，一个形状放在另一个形状的上下左右的一个合理的位置，其实就是一个多维的分类问题，根据长宽高还有缺口的宽度，我们可以预测形状应该摆放的位置
2. **可能存在数据类似的问题**：由于数据可能是存在枚举的情况，也就是相当于拟合了train数据后，拟合vali数据就基本一样了。但是采用了1000样本后，其训练结果还是类似的，至少有(250x250x4)x(250x250x2)个组合情况，降低精度也有(25x25x4)x(25x25x2)=2500*1250个组合，不可能枚举的，只是学习了组合的模式
3. **验证了样片组合的可能性**：人脑对两个样片的组合思路和计算机的组合是类似，根据边界的情况可以直接获得一个大致的模型
4. **需要进行两个样片的优化**：排样的最后结果只是大致预测出的方向，没有预测出精准的位置，所以需要在这个方向上寻找一个合适的位置，比如边界点之类的
5. **学习给出的样片组合**：如果给出了几个样片，Ax4, Bx5, Dx10这样子，然后8个样片的组合有非常多个，可以采用其中的部分模型进行学习，然后给出8个样片可以直接知道其大致的组合情况
6. **学习未见过的模型**：基本类似，可以对部分组合情况进行学习（大规模的没有比较好的结果），然后通过输入几个样片，可以直接给出最终的基本布局，该布局可以通过overlap remove的一些算法进行初始处理，然后通过local search的一些方法进行优化

## 11.26 训练记录

### 模型相关

#### Batch选择

1. Batch_Size 太小，算法在 200 epoches 内不收敛。
2. 随着 Batch_Size 增大，处理相同数据量的速度越快。
3. 随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。
4. 由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到**时间上**的最优。
5. 由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到某些时候，达到最终收敛**精度上**的最优。

**可不可以选择一个适中的 Batch_Size 值呢？**如果数据集足够充分，那么用一半（甚至少得多）的数据训练算出来的梯度与用全部数据训练出来的梯度是几乎一样的。

**在合理范围内，增大 Batch_Size 有何好处？**

- 内存利用率提高了，大矩阵乘法的并行化效率提高。
- 跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。
- 在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小。

**盲目增大 Batch_Size 有何坏处？**

- 内存利用率提高了，但是内存容量可能撑不住了。
- 跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。
- Batch_Size 增大到一定程度，其确定的下降方向已经基本不再变化。

#### 隐藏层与隐藏单元

1、隐藏单元的数量不应该超过输入层中单元的两倍（M. J. A. Berry and G. S. Linoff, Data Mining Techniques: For Marketing, Sales, and Customer Support, New York: John Wiley & Sons, 1997.）

2、隐藏单元的大小应该介于输入单元和输出单元之间（A. Blum, Neural Networks in C++, New York: Wiley, 1992.）

3、神经元的数量应捕获输入数据集方差的70~90%（Z. Boger and H. Guterman, "Knowledge extraction from artificial neural network models," in Systems, Man, and Cybernetics, 1997. Computational Cybernetics and Simulation., 1997 IEEE International Conference, Orlando, FL, 1997.）

### 18:00 重建结果分析

#### 重建结果

**预期结果**

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g9bnzb8wzbj30u00vjdgs.jpg" alt="messigray" style="zoom:30%;" />

**实际结果**

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g9bo0l6a2mj30u00u03z2.jpg" alt="messigray" style="zoom:33%;" />

#### 结果分析

1. 预期的结果和实际的结果差异性非常大
2. 类似的数据组合的预测结果类似（类似形状输入）
3. 虽然统一了数据集大小，但是输入形状的差异性非常大
4. 输入0-255和256-511的值都是绑定的，没有扰动

#### 优化方案

1. 所有的输入内容可以采用随机形状，但是相同和类似尺寸，有凹有凸
2. 生成形状-形状组合-训练集合-比较小一点的规模数据就可以了-尽快生成和测试
3. 今晚搞定！！！加油！！！！

### 15:00 数据刷新训练

#### 数据刷新后训练

256-128-32-2网络重新训练

![1000-sigmoid-11-26](https://tva1.sinaimg.cn/large/006y8mN6gy1g9bnxg00guj31e30u0jvd.jpg)

去除99999数据（采纳）

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g9bnk8cadej314p0u0n1x.jpg" alt="300-sigmoid-double-11-26" style="zoom:47%;" />

Vector部分取值为负全部变正（不采用）

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g9bnjb329mj31by0u0tcp.jpg" alt="500-clean-999-11-26" style="zoom:40%;" />

#### 训练分析

没什么好说的，数据标准化没问题，网络来看也是问题不大，整体就是拟合一个线性回归的方程，但是数据训练本身有问题

## 11.25训练记录

### 23:30训练结果

#### 结果分析



<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g9apupo9rsj319a0u00x1.jpg" alt="500-11-25-acc" style="zoom:33%;" />

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g9apunj2pzj318u0u0jw1.jpg" alt="500-11-25" style="zoom:33%;" />

### 22:30训练结果

#### 优化思路

主要可以通过以下几个方案优化，优先尝试模型预测情况，然后优化网络，如果还是不行，再去思考怎么生成训练集合，周二搞定，加油你可以的！

- [ ] 保存模型并直接测试结果情况
- [ ] 优化网络结构和损失函数
- [ ] 训练集的负值的去除处理

- [ ] 重新建立数据集，随机生成合理数据并进行排样

#### 数据集来源

EURO的部分数据组合结果，约10000条数据

#### 结果分析

1. 全部采用sigmoid，三层
2. 数据进行归一化处理
3. 训练结果大概在500左右达到最优，后续过拟合
4. 整体的验证集的准确率比较低

<img src="/Users/sean/Documents/Projects/Algorithm Learn/Packing Algorithm/picture/3000-11-25.png" alt="3000-11-25" style="zoom:28%;" />

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g9apeyimdmj318z0u0q6m.jpg" alt="3000-11-25-acc" style="zoom:40%;" />

## 11.10训练记录

### 数据集来源

随机生成数据，一个缺角的长方形和比较小的长方形和正方形，可以直接预测最合适的位置

### 模型分析

本质上来说，就是一个线性函数，比较简单就能够拟合出结果

### 训练结果

![B94F1EE1-9439-43F1-B4CB-FDB8A11E065A_1_102_o](/Users/sean/Pictures/照片图库.photoslibrary/resources/derivatives/B/B94F1EE1-9439-43F1-B4CB-FDB8A11E065A_1_102_o.jpeg)

![F5577822-0378-4647-8D4B-72513407A813_1_105_c](https://tva1.sinaimg.cn/large/006y8mN6gy1g9aptgcpwbj30v50jggq1.jpg)