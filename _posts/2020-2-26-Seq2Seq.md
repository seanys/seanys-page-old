---
layout: post
title: "Seq2Seq及其相关知识"
subtitle: "Some intorduction to Sequence to Sequence/Attention Mechanism....."
author: "sean"
header-img: "img/post-bg-ai.jpeg"
tags:
  - Deep Learning


---

### 

## 使用方式

### Keras





## 原理

Encode/Decode：如何编码和解码，比如对于Word2Vec以及其逆向操作

RNN/LSTM/GRU：基础的网络，可以对序列进行处理

Sequence to Sequence：输入一个序列，获得一个新的序列，有Pointer Network等衍生网络

Attention Mechanism：注意力机制

![preview](https://pic2.zhimg.com/v2-40ea59a2e48e803e75c4ca055d59bd7d_r.jpg)

#### 网络经验参数

$ m=\sqrt{n+l}+\alpha $

$m=log_2n$

$m=\sqrt{nl}$

m：隐藏层节点

n：输入层节点数

l：输出层节点数

$\alpha$：1到10之间的常数









