---

layout: post
title: "æœºå™¨å­¦ä¹ ä¸æ•°æ®æŒ–æ˜ç®—æ³•"
subtitle: 'Machine Learning and Data Mining Algorithm'
author: "sean"
header-img: "img/post-bg-ai.jpeg"
tags:
  - ç®—æ³•
  - æœºå™¨å­¦ä¹ 
  - æ•°æ®æŒ–æ˜
---



> âœ…  å·²ç»æ•´ç†å¥½
>
> ğŸ’¤  å¤§éƒ¨åˆ†æ•´ç†å¥½äº†ä½†æ˜¯æ²¡ç†è§£
>
> ğŸ†˜  è¿˜æœ‰ä¸€éƒ¨åˆ†éœ€è¦æ•´ç†
>
> â˜£ï¸  éœ€è¦æ•´ç†

å¤‡æ³¨ï¼šéƒ¨åˆ†ç®—æ³•åœ¨å¤šä¸ªåˆ†ç±»ä¸­å‡æœ‰åº”ç”¨ï¼Œå¯èƒ½åªå†™åœ¨ä¸€ä¸ªåˆ†ç±»ä¸­ï¼Œæˆ–è€…æ”¾åœ¨äº†å›å½’æ¨¡å‹ä¸­ï¼Œç½‘é¡µç‰ˆä¸æ”¯æŒå…¬å¼è¯­å¥ï¼Œé‡‡ç”¨å›¾ç‰‡æ¨¡å¼


## æœºå™¨å­¦ä¹ ä¸æ•°æ®æŒ–æ˜

### âœ… å†³ç­–æ ‘ï¼ˆDecision Treeï¼‰

#### âœ… å‰è¨€

**åŸºæœ¬æ¦‚å¿µ**

ï¼ˆ1ï¼‰æ ¹ç»“ç‚¹(Root Node)ï¼šå®ƒè¡¨ç¤ºæ•´ä¸ªæ ·æœ¬é›†åˆï¼Œå¹¶ä¸”è¯¥èŠ‚ç‚¹å¯ä»¥è¿›ä¸€æ­¥åˆ’åˆ†æˆä¸¤ä¸ªæˆ–å¤šä¸ªå­é›†ã€‚

ï¼ˆ2ï¼‰æ‹†åˆ†(Splitting)ï¼šè¡¨ç¤ºå°†ä¸€ä¸ªç»“ç‚¹æ‹†åˆ†æˆå¤šä¸ªå­é›†çš„è¿‡ç¨‹ã€‚

ï¼ˆ3ï¼‰å†³ç­–ç»“ç‚¹(Decision Node)ï¼šå½“ä¸€ä¸ªå­ç»“ç‚¹è¿›ä¸€æ­¥è¢«æ‹†åˆ†æˆå¤šä¸ªå­èŠ‚ç‚¹æ—¶ï¼Œè¿™ä¸ªå­èŠ‚ç‚¹å°±å«åšå†³ç­–ç»“ç‚¹ã€‚

ï¼ˆ4ï¼‰å¶å­ç»“ç‚¹(Leaf/Terminal Node)ï¼šæ— æ³•å†æ‹†åˆ†çš„ç»“ç‚¹è¢«ç§°ä¸ºå¶å­ç»“ç‚¹ã€‚

ï¼ˆ5ï¼‰å‰ªæ(Pruning)ï¼šç§»é™¤å†³ç­–æ ‘ä¸­å­ç»“ç‚¹çš„è¿‡ç¨‹å°±å«åšå‰ªæï¼Œè·Ÿæ‹†åˆ†è¿‡ç¨‹ç›¸åã€‚

ï¼ˆ6ï¼‰åˆ†æ”¯/å­æ ‘(Branch/Sub-Tree)ï¼šä¸€æ£µå†³ç­–æ ‘çš„ä¸€éƒ¨åˆ†å°±å«åšåˆ†æ”¯æˆ–å­æ ‘ã€‚

ï¼ˆ7ï¼‰çˆ¶ç»“ç‚¹å’Œå­ç»“ç‚¹(Paren and Child Node)ï¼šä¸€ä¸ªç»“ç‚¹è¢«æ‹†åˆ†æˆå¤šä¸ªå­èŠ‚ç‚¹ï¼Œè¿™ä¸ªç»“ç‚¹å°±å«åšçˆ¶èŠ‚ç‚¹ï¼›å…¶æ‹†åˆ†åçš„å­ç»“ç‚¹ä¹Ÿå«åšå­ç»“ç‚¹ã€‚

**åˆ†ç±»**

- ç¦»æ•£æ€§å†³ç­–æ ‘ï¼šç¦»æ•£æ€§å†³ç­–æ ‘ï¼Œå…¶ç›®æ ‡å˜é‡æ˜¯ç¦»æ•£çš„ï¼Œå¦‚æ€§åˆ«ï¼šç”·æˆ–å¥³ç­‰ï¼›
- è¿ç»­æ€§å†³ç­–æ ‘ï¼šè¿ç»­æ€§å†³ç­–æ ‘ï¼Œå…¶ç›®æ ‡å˜é‡æ˜¯è¿ç»­çš„ï¼Œå¦‚å·¥èµ„ã€ä»·æ ¼ã€å¹´é¾„ç­‰ï¼›

![img](https://shuwoom.com/wp-content/uploads/2018/10/65e4301fe9e9a0f4e4d448a8f4181751.png)

**ç‰¹å¾é€‰æ‹©**

ç‰¹å¾é€‰æ‹©è¡¨ç¤ºä»ä¼—å¤šçš„ç‰¹å¾ä¸­é€‰æ‹©ä¸€ä¸ªç‰¹å¾ä½œä¸ºå½“å‰èŠ‚ç‚¹åˆ†è£‚çš„æ ‡å‡†ï¼Œå¦‚ä½•é€‰æ‹©ç‰¹å¾æœ‰ä¸åŒçš„é‡åŒ–è¯„ä¼°æ–¹æ³•ï¼Œä»è€Œè¡ç”Ÿå‡ºä¸åŒçš„å†³ç­–æ ‘ï¼Œå¦‚ID3ï¼ˆé€šè¿‡ä¿¡æ¯å¢ç›Šé€‰æ‹©ç‰¹å¾ï¼‰ã€C4.5ï¼ˆé€šè¿‡ä¿¡æ¯å¢ç›Šæ¯”é€‰æ‹©ç‰¹å¾ï¼‰ã€CARTï¼ˆé€šè¿‡GiniæŒ‡æ•°é€‰æ‹©ç‰¹å¾ï¼‰ç­‰ã€‚

**å†³ç­–æ ‘çš„ç”Ÿæˆ**

æ ¹æ®é€‰æ‹©çš„ç‰¹å¾è¯„ä¼°æ ‡å‡†ï¼Œä»ä¸Šè‡³ä¸‹é€’å½’åœ°ç”Ÿæˆå­èŠ‚ç‚¹ï¼Œç›´åˆ°æ•°æ®é›†ä¸å¯åˆ†åˆ™åœæ­¢å†³ç­–æ ‘åœæ­¢ç”Ÿé•¿ã€‚è¿™ä¸ªè¿‡ç¨‹å®é™…ä¸Šå°±æ˜¯ä½¿ç”¨æ»¡è¶³åˆ’åˆ†å‡†åˆ™çš„ç‰¹å¾ä¸æ–­çš„å°†æ•°æ®é›†åˆ’åˆ†æˆçº¯åº¦æ›´é«˜ï¼Œä¸ç¡®å®šè¡Œæ›´å°çš„å­é›†çš„è¿‡ç¨‹ã€‚å¯¹äºå½“å‰æ•°æ®é›†çš„æ¯ä¸€æ¬¡åˆ’åˆ†ï¼Œéƒ½å¸Œæœ›æ ¹æ®æŸä¸ªç‰¹å¾åˆ’åˆ†ä¹‹åçš„å„ä¸ªå­é›†çš„çº¯åº¦æ›´é«˜ï¼Œä¸ç¡®å®šæ€§æ›´å°ã€‚

**å†³ç­–æ ‘çš„è£å‰ª**

å†³ç­–æ ‘å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œä¸€èˆ¬éœ€è¦å‰ªææ¥ç¼©å°æ ‘ç»“æ„è§„æ¨¡ã€ç¼“è§£è¿‡æ‹Ÿåˆã€‚

å‚è€ƒèµ„æ–™ï¼šhttps://shuwoom.com/?p=1452

#### âœ… ID3ç®—æ³•

ç†µè¿™ä¸ªæ¦‚å¿µæœ€æ—©èµ·æºäºç‰©ç†å­¦ï¼Œåœ¨ç‰©ç†å­¦ä¸­æ˜¯ç”¨æ¥åº¦é‡ä¸€ä¸ªçƒ­åŠ›å­¦ç³»ç»Ÿçš„æ— åºç¨‹åº¦ï¼Œè€Œåœ¨ä¿¡æ¯å­¦é‡Œé¢ï¼Œç†µæ˜¯å¯¹ä¸ç¡®å®šæ€§çš„åº¦é‡ã€‚åœ¨1948å¹´ï¼Œé¦™å†œå¼•å…¥äº†ä¿¡æ¯ç†µï¼ˆinformation entropyï¼‰ï¼Œå°†å…¶å®šä¹‰ä¸ºç¦»æ•£éšæœºäº‹ä»¶å‡ºç°çš„æ¦‚ç‡ï¼Œä¸€ä¸ªç³»ç»Ÿè¶Šæ˜¯æœ‰åºï¼Œä¿¡æ¯ç†µå°±è¶Šä½ï¼Œåä¹‹ä¸€ä¸ªç³»ç»Ÿè¶Šæ˜¯æ··ä¹±ï¼Œå®ƒçš„ä¿¡æ¯ç†µå°±è¶Šé«˜ã€‚æ‰€ä»¥ä¿¡æ¯ç†µå¯ä»¥è¢«è®¤ä¸ºæ˜¯ç³»ç»Ÿæœ‰åºåŒ–ç¨‹åº¦çš„ä¸€ä¸ªåº¦é‡ã€‚

**æ¡ˆä¾‹**ï¼šå¯¹äºæœ‰Kä¸ªç±»åˆ«çš„åˆ†ç±»é—®é¢˜æ¥è¯´ï¼Œå‡å®šæ ·æœ¬é›†åˆDä¸­ç¬¬ k ç±»æ ·æœ¬æ‰€å çš„æ¯”ä¾‹ä¸ºpkï¼ˆk=1,2,...,Kï¼‰,åˆ™æ ·æœ¬é›†åˆDçš„ä¿¡æ¯ç†µå®šä¹‰ä¸º: 

<img src="https://tva1.sinaimg.cn/large/006y8mN6ly1g83aclz7lhj30d602it8q.jpg" alt="image-20191019100957943" style="zoom:50%;" />

**ç®€å•æ¦‚æ‹¬**ï¼šåˆ†å‰æ ‘ä¸Šçš„ç†µçš„åŠ æƒå¹³å‡å€¼

```python
from math import log
import operator

def calcShannonEnt(dataSet):  # è®¡ç®—æ•°æ®çš„ç†µ(entropy)
    numEntries=len(dataSet)  # æ•°æ®æ¡æ•°
    labelCounts={}
    for featVec in dataSet:
        currentLabel=featVec[-1] # æ¯è¡Œæ•°æ®çš„æœ€åä¸€ä¸ªå­—ï¼ˆç±»åˆ«ï¼‰
        if currentLabel not in labelCounts.keys():
            labelCounts[currentLabel]=0
        labelCounts[currentLabel]+=1  # ç»Ÿè®¡æœ‰å¤šå°‘ä¸ªç±»ä»¥åŠæ¯ä¸ªç±»çš„æ•°é‡
    shannonEnt=0
    for key in labelCounts:
        prob=float(labelCounts[key])/numEntries # è®¡ç®—å•ä¸ªç±»çš„ç†µå€¼
        shannonEnt-=prob*log(prob,2) # ç´¯åŠ æ¯ä¸ªç±»çš„ç†µå€¼
    return shannonEnt

def createDataSet1():    # åˆ›é€ ç¤ºä¾‹æ•°æ®
    dataSet = [['é•¿', 'ç²—', 'ç”·'],
               ['çŸ­', 'ç²—', 'ç”·'],
               ['çŸ­', 'ç²—', 'ç”·'],
               ['é•¿', 'ç»†', 'å¥³'],
               ['çŸ­', 'ç»†', 'å¥³'],
               ['çŸ­', 'ç²—', 'å¥³'],
               ['é•¿', 'ç²—', 'å¥³'],
               ['é•¿', 'ç²—', 'å¥³']]
    labels = ['å¤´å‘','å£°éŸ³']  #ä¸¤ä¸ªç‰¹å¾
    return dataSet,labels

def splitDataSet(dataSet,axis,value): # æŒ‰æŸä¸ªç‰¹å¾åˆ†ç±»åçš„æ•°æ®
    retDataSet=[]
    for featVec in dataSet:
        if featVec[axis]==value:
            reducedFeatVec =featVec[:axis]
            reducedFeatVec.extend(featVec[axis+1:])
            retDataSet.append(reducedFeatVec)
    return retDataSet

def chooseBestFeatureToSplit(dataSet):  # é€‰æ‹©æœ€ä¼˜çš„åˆ†ç±»ç‰¹å¾
    numFeatures = len(dataSet[0])-1
    baseEntropy = calcShannonEnt(dataSet)  # åŸå§‹çš„ç†µ
    bestInfoGain = 0
    bestFeature = -1
    for i in range(numFeatures):
        featList = [example[i] for example in dataSet]
        uniqueVals = set(featList)
        newEntropy = 0
        for value in uniqueVals:
            subDataSet = splitDataSet(dataSet,i,value)
            prob =len(subDataSet)/float(len(dataSet))
            newEntropy +=prob*calcShannonEnt(subDataSet)  # æŒ‰ç‰¹å¾åˆ†ç±»åçš„ç†µ
        infoGain = baseEntropy - newEntropy  # åŸå§‹ç†µä¸æŒ‰ç‰¹å¾åˆ†ç±»åçš„ç†µçš„å·®å€¼
        if (infoGain>bestInfoGain):   # è‹¥æŒ‰æŸç‰¹å¾åˆ’åˆ†åï¼Œç†µå€¼å‡å°‘çš„æœ€å¤§ï¼Œåˆ™æ¬¡ç‰¹å¾ä¸ºæœ€ä¼˜åˆ†ç±»ç‰¹å¾
            bestInfoGain=infoGain
            bestFeature = i
    return bestFeature

def majorityCnt(classList):    #æŒ‰åˆ†ç±»åç±»åˆ«æ•°é‡æ’åºï¼Œæ¯”å¦‚ï¼šæœ€ååˆ†ç±»ä¸º2ç”·1å¥³ï¼Œåˆ™åˆ¤å®šä¸ºç”·ï¼›
    classCount={}
    for vote in classList:
        if vote not in classCount.keys():
            classCount[vote]=0
        classCount[vote]+=1
    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)
    return sortedClassCount[0][0]

def createTree(dataSet,labels):
    classList=[example[-1] for example in dataSet]  # ç±»åˆ«ï¼šç”·æˆ–å¥³
    if classList.count(classList[0])==len(classList):
        return classList[0]
    if len(dataSet[0])==1:
        return majorityCnt(classList)
    bestFeat=chooseBestFeatureToSplit(dataSet) #é€‰æ‹©æœ€ä¼˜ç‰¹å¾
    bestFeatLabel=labels[bestFeat]
    myTree={bestFeatLabel:{}} #åˆ†ç±»ç»“æœä»¥å­—å…¸å½¢å¼ä¿å­˜
    del(labels[bestFeat])
    featValues=[example[bestFeat] for example in dataSet]
    uniqueVals=set(featValues)
    for value in uniqueVals:
        subLabels=labels[:]
        myTree[bestFeatLabel][value]=createTree(splitDataSet\
                            (dataSet,bestFeat,value),subLabels)
    return myTree


if __name__=='__main__':
    dataSet, labels=createDataSet1()  # åˆ›é€ ç¤ºåˆ—æ•°æ®
    print(createTree(dataSet, labels))  # è¾“å‡ºå†³ç­–æ ‘æ¨¡å‹ç»“æœ
```

#### âœ… C4.5ç®—æ³•

**èƒŒæ™¯**ï¼šæˆ‘ä»¬çŸ¥é“ä¿¡æ¯å¢ç›Šä¼šåå‘å–å€¼è¾ƒå¤šçš„ç‰¹å¾ï¼Œä½¿ç”¨ä¿¡æ¯å¢ç›Šæ¯”å¯ä»¥å¯¹è¿™ä¸€é—®é¢˜è¿›è¡Œæ ¡æ­£ã€‚

**ä¿¡æ¯å¢ç›Š**ï¼šGain(D,A) = H(D) â€“ H(D|A)ï¼Œç”¨äºID3çš„åˆ¤æ–­

**ä¿¡æ¯å¢ç›Šæ¯”**ï¼šç‰¹å¾Aå¯¹è®­ç»ƒæ•°æ®é›†Dçš„ä¿¡æ¯å¢ç›Šæ¯”GainRatio(D,A)å®šä¹‰ä¸ºå…¶ä¿¡æ¯å¢ç›ŠGain(D,A)ä¸è®­ç»ƒæ•°æ®é›†Dçš„ç»éªŒç†µH(D)ä¹‹æ¯”ï¼š

<img src="https://shuwoom.com/wp-content/uploads/2018/10/aae983046e32f250faf6daaaf376ec90.png" alt="img" style="zoom:50%;" />

```python
def choose_best_feature_to_split(data_set):
    """
    æŒ‰ç…§æœ€å¤§ä¿¡æ¯å¢ç›Šæ¯”åˆ’åˆ†æ•°æ®
    :param data_set: æ ·æœ¬æ•°æ®ï¼Œå¦‚ï¼š [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]
    :return:
    """
    num_feature = len(data_set[0]) - 1 # ç‰¹å¾ä¸ªæ•°ï¼Œå¦‚ï¼šä¸æµ®å‡ºæ°´é¢æ˜¯å¦å¯ä»¥ç”Ÿå­˜	å’Œæ˜¯å¦æœ‰è„šè¹¼
    base_entropy = calc_shannon_ent(data_set) # ç»éªŒç†µH(D)
    best_info_gain_ratio = 0.0
    best_feature_idx = -1
    for feature_idx in range(num_feature):
        feature_val_list = [number[feature_idx] for number in data_set]  # å¾—åˆ°æŸä¸ªç‰¹å¾ä¸‹æ‰€æœ‰å€¼ï¼ˆæŸåˆ—ï¼‰
        unique_feature_val_list = set(feature_val_list)  # è·å–æ— é‡å¤çš„å±æ€§ç‰¹å¾å€¼
        new_entropy = 0
        split_info = 0.0
        for value in unique_feature_val_list:
            sub_data_set = split_data_set(data_set, feature_idx, value)
            prob = len(sub_data_set) / float(len(data_set))  # å³p(t)
            new_entropy += prob * calc_shannon_ent(sub_data_set)  # å¯¹å„å­é›†é¦™å†œç†µæ±‚å’Œ
            split_info += -prob * log(prob, 2)
        info_gain = base_entropy - new_entropy  # è®¡ç®—ä¿¡æ¯å¢ç›Šï¼Œg(D,A)=H(D)-H(D|A)
        if split_info == 0:  # fix the overflow bug
            continue
        info_gain_ratio = info_gain / split_info
        # æœ€å¤§ä¿¡æ¯å¢ç›Šæ¯”
        if info_gain_ratio > best_info_gain_ratio:
            best_info_gain_ratio = info_gain_ratio
            best_feature_idx = feature_idx

    return best_feature_idx
```

#### âœ… CARTç®—æ³•

**æ¦‚è¿°**ï¼šCARTåˆ†ç±»æ ‘é¢„æµ‹åˆ†ç±»ç¦»æ•£å‹æ•°æ®ï¼Œé‡‡ç”¨åŸºå°¼æŒ‡æ•°é€‰æ‹©æœ€ä¼˜ç‰¹å¾ï¼ŒåŒæ—¶å†³å®šè¯¥ç‰¹å¾çš„æœ€ä¼˜äºŒå€¼åˆ‡åˆ†ç‚¹ã€‚

**æ¦‚ç‡åˆ†å¸ƒçš„åŸºå°¼æŒ‡æ•°å®šä¹‰**ï¼šå‡è®¾æœ‰Kä¸ªç±»ï¼Œæ ·æœ¬ç‚¹å±äºç¬¬kä¸ªç±»çš„æ¦‚ç‡ä¸ºPk

![img](https://pic3.zhimg.com/80/v2-780d955260d9a2ba8508c1601588b88a_hd.jpg)

**é›†åˆDçš„åŸºå°¼æŒ‡æ•°**ï¼š

![img](https://pic2.zhimg.com/80/v2-95300197189b4b1b65eb42d1a6bbd7fd_hd.jpg)

**è¿‡ç¨‹æ¦‚è¿°**ï¼šè®¡ç®—åŸºå°¼ç³»æ•°çš„åŠ æƒå¹³å‡ï¼Œæ±‚min

![img](https://shuwoom.com/wp-content/uploads/2018/10/fb6de7e8b17762b1caa14d6968407289.png)

![img](https://shuwoom.com/wp-content/uploads/2018/10/251158892a1340a1829e3a9f23dc477b.png)

```python
# -*- coding: utf-8 -*-
import numpy as np

class Tree(object):
    def __init__(self, value=None, true_branch=None, false_branch=None, results=None, col=-1, summary=None, data=None):
        self.value = value
        self.true_branch = true_branch
        self.false_branch = false_branch
        self.results = results
        self.col = col
        self.summary = summary
        self.data = data

    def __str__(self):
        print(self.col, self.value)
        print(self.results)
        print(self.summary)
        return ""

def split_datas(rows, value, column):
    """
    æ ¹æ®æ¡ä»¶åˆ†ç¦»æ•°æ®é›†
    :param rows:
    :param value:
    :param column:
    :return:  (list1, list2)
    """
    list1 = []
    list2 = []
    if isinstance(value, int) or isinstance(value, float):
        for row in rows:
            if row[column] >= value:
                list1.append(row)
            else:
                list2.append(row)
    else:
        for row in rows:
            if row[column] == value:
                list1.append(row)
            else:
                list2.append(row)

    return list1, list2


def calculate_diff_count(data_set):
    """
    åˆ†ç±»ç»Ÿè®¡data_setä¸­æ¯ä¸ªç±»åˆ«çš„æ•°é‡
    :param datas:å¦‚ï¼š[[5.1, 3.5, 1.4, 0.2, 'setosa'], [4.9, 3, 1.4, 0.2, 'setosa'],....]
    :return: å¦‚ï¼š{'setosa': 50, 'versicolor': 50, 'virginica': 50}
    """
    results = {}
    for data in data_set:
        # æ•°æ®çš„æœ€åä¸€åˆ—data[-1]æ˜¯ç±»åˆ«
        if data[-1] not in results:
            results.setdefault(data[-1], 1)
        else:
            results[data[-1]] += 1
    return results


def gini(data_set):
    """
    è®¡ç®—giniçš„å€¼ï¼Œå³Gini(p)
    :param data_set: å¦‚ï¼š[[5.1, 3.5, 1.4, 0.2, 'setosa'], [4.9, 3, 1.4, 0.2, 'setosa'],....]
    :return:
    """
    length = len(data_set)
    category_2_cnt = calculate_diff_count(data_set)
    sum = 0.0
    for category in category_2_cnt:
        sum += pow(float(category_2_cnt[category]) / length, 2)
    return 1 - sum


def build_decision_tree(data_set, evaluation_function=gini):
    """
    é€’å½’å»ºç«‹å†³ç­–æ ‘ï¼Œå½“gain=0æ—¶ï¼Œåœæ­¢å›å½’
    :param data_set: å¦‚ï¼š[[5.1, 3.5, 1.4, 0.2, 'setosa'], [4.9, 3, 1.4, 0.2, 'setosa'],....]
    :param evaluation_function:
    :return:
    """
    current_gain = evaluation_function(data_set)
    column_length = len(data_set[0])
    rows_length = len(data_set)

    best_gain = 0.0
    best_value = None
    best_set = None

    # choose the best gain
    for feature_idx in range(column_length - 1):
        feature_value_set = set(row[feature_idx] for row in data_set)
        for feature_value in feature_value_set:
            sub_data_set1, sub_data_set2 = split_datas(data_set, feature_value, feature_idx)
            p = float(len(sub_data_set1)) / rows_length
            # Gini(D,A)è¡¨ç¤ºåœ¨ç‰¹å¾Açš„æ¡ä»¶ä¸‹é›†åˆDçš„åŸºå°¼æŒ‡æ•°ï¼Œgini_d_aè¶Šå°ï¼Œæ ·æœ¬é›†åˆä¸ç¡®å®šæ€§è¶Šå°
            # æˆ‘ä»¬çš„ç›®çš„æ˜¯æ‰¾åˆ°å¦gini_d_aæœ€å°çš„ç‰¹å¾ï¼ŒåŠgainæœ€å¤§çš„ç‰¹å¾
            gini_d_a = p * evaluation_function(sub_data_set1) + (1 - p) * evaluation_function(sub_data_set2)
            gain = current_gain - gini_d_a
            if gain > best_gain:
                best_gain = gain
                best_value = (feature_idx, feature_value)
                best_set = (sub_data_set1, sub_data_set2)
    dc_y = {'impurity': '%.3f' % current_gain, 'sample': '%d' % rows_length}

    # stop or not stop
    if best_gain > 0:
        true_branch = build_decision_tree(best_set[0], evaluation_function)
        false_branch = build_decision_tree(best_set[1], evaluation_function)
        return Tree(col=best_value[0], value=best_value[1], true_branch=true_branch, false_branch=false_branch, summary=dc_y)
    else:
        return Tree(results=calculate_diff_count(data_set), summary=dc_y, data=data_set)


def prune(tree, mini_gain, evaluation_function=gini):
    """
    è£å‰ª
    :param tree:
    :param mini_gain:
    :param evaluation_function:
    :return:
    """
    if tree.true_branch.results == None:
        prune(tree.true_branch, mini_gain, evaluation_function)
    if tree.false_branch.results == None:
        prune(tree.false_branch, mini_gain, evaluation_function)

    if tree.true_branch.results != None and tree.false_branch.results != None:
        len1 = len(tree.true_branch.data)
        len2 = len(tree.false_branch.data)
        len3 = len(tree.true_branch.data + tree.false_branch.data)

        p = float(len1) / (len1 + len2)

        gain = evaluation_function(tree.true_branch.data + tree.false_branch.data) \
               - p * evaluation_function(tree.true_branch.data)\
               - (1 - p) * evaluation_function(tree.false_branch.data)

        if gain < mini_gain:
            # å½“èŠ‚ç‚¹çš„gainå°äºç»™å®šçš„ mini Gainæ—¶åˆ™åˆå¹¶è¿™ä¸¤ä¸ªèŠ‚ç‚¹
            tree.data = tree.true_branch.data + tree.false_branch.data
            tree.results = calculate_diff_count(tree.data)
            tree.true_branch = None
            tree.false_branch = None


def classify(data, tree):
    """
    åˆ†ç±»
    :param data:
    :param tree:
    :return:
    """
    if tree.results != None:
        return tree.results
    else:
        branch = None
        v = data[tree.col]
        if isinstance(v, int) or isinstance(v, float):
            if v >= tree.value:
                branch = tree.true_branch
            else:
                branch = tree.false_branch
        else:
            if v == tree.value:
                branch = tree.true_branch
            else:
                branch = tree.false_branch
        return classify(data, branch)


def load_csv():
    def convert_types(s):
        s = s.strip()
        try:
            return float(s) if '.' in s else int(s)
        except ValueError:
            return s
    data = np.loadtxt("datas.csv", dtype="str", delimiter=",")
    data = data[1:, :]
    data_set = ([[convert_types(item) for item in row] for row in data])
    return data_set


if __name__ == '__main__':
    data_set = load_csv()
    print data_set
    decistion_tree = build_decision_tree(data_set, evaluation_function=gini)
    print decistion_tree.results
    # prune(decistion_tree, 0.4)
    print classify([5.1,3.5,1.4,0.2], decistion_tree) # setosa
    print classify([6.8,2.8,4.8,1.4], decistion_tree) # versicolor
    print classify([6.8,3.2,5.9,2.3], decistion_tree) # virginica
```

### â˜£ï¸ KNNåˆ†ç±»



### â˜£ï¸ PCAæ¨¡å‹



### â˜£ï¸ EMç®—æ³•



### â˜£ï¸ Aprioriç®—æ³•



### â˜£ï¸ æœ€å¤§æœŸæœ›ç®—æ³•



### â˜£ï¸ K-Meansç®—æ³•



### â˜£ï¸ SVMç®—æ³•



### â˜£ï¸ Adaboostç®—æ³•



### â˜£ï¸ è´å¶æ–¯æ¨¡å‹ï¼ˆNaive Bayesï¼‰

```python
#!/usr/bin/python
# -*- coding: utf-8 -*-
#########################################
# Bayes : ç”¨æ¥æè¿°ä¸¤ä¸ªæ¡ä»¶æ¦‚ç‡ä¹‹é—´çš„å…³ç³»

# å‚æ•°:        inX: vector to compare to existing dataset (1xN)
#             dataSet: size m data set of known vectors (NxM)
#             labels: data set labels (1xM vector)
#             å…¬å¼ï¼šP(A|B)=P(B|A)P(A)/P(B)
# è¾“å‡º:       å‡ºé”™ç‡
#########################################

import numpy as npy
import os
import time

#P(B|A)=P(A|B)*P(A)/P(B)

# æ•°æ®é›†ç›®å½•
dataSetDir ='/Users/sean/Datasets'

class Bayes:
    def __init__(self):
        self.length=-1
        self.labelrate=dict()
        self.vectorrate=dict()

    def fit(self,dataset:list,labels:list):
        print("è®­ç»ƒå¼€å§‹")
        if len(dataset)!=len(labels):
            raise ValueError("è¾“å…¥æµ‹è¯•æ•°ç»„å’Œç±»åˆ«æ•°ç»„é•¿åº¦ä¸ä¸€è‡´")
        self.length=len(dataset[0])#è®­ç»ƒæ•°æ®ç‰¹å¾å€¼çš„é•¿åº¦
        labelsnum=len(labels) #ç±»åˆ«çš„æ•°é‡
        norlabels=set(labels) #ä¸é‡å¤ç±»åˆ«çš„æ•°é‡
        for item in norlabels:
            self.labelrate[item]=labels.count(item)/labelsnum #æ±‚å½“å‰ç±»åˆ«å æ€»ç±»åˆ«çš„æ¯”ä¾‹
        for vector,label in zip(dataset,labels):
            if label not in self.vectorrate:
                self.vectorrate[label]=[]
            self.vectorrate[label].append(vector)
        print("è®­ç»ƒç»“æŸ")
        return self

    def btest(self,testdata,labelset):
        if self.length==-1:
            raise ValueError("æœªå¼€å§‹è®­ç»ƒï¼Œå…ˆè®­ç»ƒ")
        #è®¡ç®—testdataåˆ†åˆ«ä¸ºå„ä¸ªç±»åˆ«çš„æ¦‚ç‡
        lbDict=dict()
        for thislb in labelset:
            p = 1
            alllabel = self.labelrate[thislb]
            allvector = self.vectorrate[thislb]
            vnum=len(allvector)
            allvector=npy.array(allvector).T
            for index in range(0,len(testdata)):
                vector=list(allvector[index])
                p*=vector.count(testdata[index])/vnum
            lbDict[thislb]=p * alllabel
        thislbabel=sorted(lbDict,key=lambda x:lbDict[x],reverse=True)[0]
        return thislbabel

#åŠ è½½æ•°æ®
def datatoarray(fname):
    arr=[]
    fh=open(fname)
    for i in range(0,32):
        thisline=fh.readline()
        for j in range(0 , 32):
            arr.append(int(thisline[j]))
    return arr

#å»ºç«‹ä¸€ä¸ªå‡½æ•°å–å‡ºlabels
def seplabel(fname):
    filestr=fname.split(".")[0]
    label=int(filestr.split("_")[0])
    return label

#å»ºç«‹è®­ç»ƒæ•°æ®
def traindata():
    labels=[]
    trainfile=os.listdir(dataSetDir+"trainingDigits") # åŠ è½½æµ‹è¯•æ•°æ®
    num=len(trainfile)
    trainarr=npy.zeros((num,1024))
    for i in range(num):
        thisfname=trainfile[i]
        thislabel=seplabel(thisfname)
        labels.append(thislabel)
        trainarr[i,]=datatoarray(dataSetDir+"trainingDigits/"+thisfname)
    return trainarr,labels

# è´å¶æ–¯ç®—æ³•æ‰‹å†™è¯†åˆ«ä¸»æµç¨‹
bys=Bayes()
start = time.time()

# # step 1: è®­ç»ƒæ•°æ®é›†
train_data,labels=traindata()
train_data=list(train_data)
bys.fit(train_data,labels)

# # step 2:æµ‹è¯•æ•°æ®é›†
thisdata=datatoarray(dataSetDir+"testDigits/8_90.txt")
labelsall=[0,1,2,3,4,5,6,7,8,9]

# # è¯†åˆ«å•ä¸ªæ‰‹å†™ä½“æ•°å­—
# test=bys.btest(thisdata,labelsall)
# print(test)

# # è¯†åˆ«å¤šä¸ªæ‰‹å†™ä½“æ•°å­—ï¼ˆæ‰¹é‡å¤„ç†ï¼‰,å¹¶è¾“å‡ºç»“æœ
testfile=os.listdir(dataSetDir+"testDigits")
num=len(testfile)
x=0
for i in range(num):
    thisfilename=testfile[i]
    thislabel=seplabel(thisfilename)
    thisdataarr=datatoarray(dataSetDir+"testDigits/"+thisfilename)
    label=bys.btest(thisdataarr,labelsall)
    print("æµ‹è¯•æ•°å­—æ˜¯ï¼š"+str(thislabel)+"  è¯†åˆ«å‡ºæ¥çš„æ•°å­—æ˜¯ï¼š"+str(label))
    if label!=thislabel:
        x+=1
        print("è¯†åˆ«å‡ºé”™")
print(x)
print("å‡ºé”™ç‡ï¼š"+str(x/num))

end = time.time()
running_time = end-start
print('ç¨‹åºè¿è¡Œæ€»è€—æ—¶ï¼š %.5f sec' %running_time)
```



## å­¦ä¹ ç½‘ç»œ

### â˜£ï¸ æ¢¯åº¦ä¸‹é™



### â˜£ï¸ BPç½‘ç»œ



### â˜£ï¸ RNNç³»åˆ—

#### â˜£ï¸ RNNæ¨¡å‹



![roolled RNN](https://caicai.science/images/attention/roolled%20rnn.png)

#### â˜£ï¸ LSTM



#### â˜£ï¸ GRU



## ç›¸å…³å†…å®¹

### â˜£ï¸ æ¢¯åº¦çˆ†ç‚¸é—®é¢˜





### â˜£ï¸ ä»£ä»·å‡½æ•°

#### âœ… äºŒæ¬¡ä»£ä»·å‡½æ•°

![img](https://img-blog.csdn.net/20160402180717102)

![img](https://img-blog.csdn.net/20160402175137034)

#### âœ… äº¤å‰ç†µæŸå¤±å‡½æ•°

![img](https://img-blog.csdn.net/20160402172100739)

å‚è€ƒèµ„æ–™ï¼šhttps://blog.csdn.net/u014313009/article/details/51043064



## å‰æ²¿ç ”ç©¶

### âœ… Sequence to Sequence

![è¿™é‡Œå†™å›¾ç‰‡æè¿°](https://img-blog.csdn.net/20171101103708217)

![img](https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806141638641-1078830067.png)

**æ¦‚è¿°**ï¼šä¸€ç§å‡½æ•°æ˜ å°„æ¨¡å‹ï¼Œå…·æœ‰å¼±çº¦æŸçš„å¤šå¯¹å¤šå¯¹åº”å…³ç³»ã€‚

**åŸç†**ï¼šå·¦ä¾§Encoderç¼–ç å°†è¾“å…¥åºåˆ—è½¬åŒ–æˆä¸€ä¸ªå›ºå®šé•¿åº¦çš„å‘é‡ç¼–ç ï¼Œå³ä¾§Decoderè§£ç å°†ä¹‹å‰ç”Ÿæˆçš„å›ºå®šå‘é‡å†è½¬åŒ–æˆè¾“å‡ºåºåˆ—ï¼Œç¼–è§£ç éƒ¨åˆ†å¯ä»¥é‡‡ç”¨CNNã€RNNã€LSTMã€GRUã€BLSTMç­‰å®ç°ã€‚å¯ä»¥é¢„æµ‹ä»»æ„çš„åºåˆ—å¯¹åº”å…³ç³»ã€‚

**æ„ä¹‰**ï¼šç¬¬ä¸€æ¬¡æå‡ºäº†Encoder-Decoderæ¨¡å‹ï¼Œä¸»è¦ç”¨äºåºåˆ—é¢„æµ‹

**é—®é¢˜**ï¼šEncoderæ˜¯å°†è¾“å…¥å†…å®¹å‹ç¼©åˆ°ä¸€ä¸ªå›ºå®šé•¿åº¦çš„å‘é‡ä¸­ï¼Œè¯­ä¹‰ç¼–ç æ˜¯å›ºå®šé•¿åº¦ï¼Œç„¶åè®¡ç®—å‡ºå†…å®¹è¿›è¡ŒDecoder

### âœ… Attention Mechanism

ã€ŠSequence to Sequence Learning with Neural Networksã€‹ä»‹ç»äº†ä¸€ç§åŸºäºRNNçš„Seq2Seqæ¨¡å‹ï¼ŒåŸºäºä¸€ä¸ªEncoderå’Œä¸€ä¸ªDecoderæ¥æ„å»ºåŸºäºç¥ç»ç½‘ç»œçš„End-to-Endçš„æœºå™¨ç¿»è¯‘æ¨¡å‹ï¼Œå…¶ä¸­ï¼ŒEncoderæŠŠè¾“å…¥Xç¼–ç æˆä¸€ä¸ªå›ºå®šé•¿åº¦çš„éšå‘é‡Zï¼ŒDecoderåŸºäºéšå‘é‡Zè§£ç å‡ºç›®æ ‡è¾“å‡ºYã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸ç»å…¸çš„åºåˆ—åˆ°åºåˆ—çš„æ¨¡å‹ï¼Œä½†æ˜¯å´å­˜åœ¨**ä¸¤ä¸ªæ˜æ˜¾çš„é—®é¢˜**ï¼š

1ã€æŠŠè¾“å…¥Xçš„æ‰€æœ‰ä¿¡æ¯æœ‰å‹ç¼©åˆ°ä¸€ä¸ªå›ºå®šé•¿åº¦çš„éšå‘é‡Zï¼Œå¿½ç•¥äº†è¾“å…¥è¾“å…¥Xçš„é•¿åº¦ï¼Œå½“è¾“å…¥å¥å­é•¿åº¦å¾ˆé•¿ï¼Œç‰¹åˆ«æ˜¯æ¯”è®­ç»ƒé›†ä¸­æœ€åˆçš„å¥å­é•¿åº¦è¿˜é•¿æ—¶ï¼Œæ¨¡å‹çš„æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚

2ã€æŠŠè¾“å…¥Xç¼–ç æˆä¸€ä¸ªå›ºå®šçš„é•¿åº¦ï¼Œå¯¹äºå¥å­ä¸­æ¯ä¸ªè¯éƒ½èµ‹äºˆç›¸åŒçš„æƒé‡ï¼Œè¿™æ ·åšæ˜¯ä¸åˆç†çš„ï¼Œæ¯”å¦‚ï¼Œåœ¨æœºå™¨ç¿»è¯‘é‡Œï¼Œè¾“å…¥çš„å¥å­ä¸è¾“å‡ºå¥å­ä¹‹é—´ï¼Œå¾€å¾€æ˜¯è¾“å…¥ä¸€ä¸ªæˆ–å‡ ä¸ªè¯å¯¹åº”äºè¾“å‡ºçš„ä¸€ä¸ªæˆ–å‡ ä¸ªè¯ã€‚å› æ­¤ï¼Œå¯¹è¾“å…¥çš„æ¯ä¸ªè¯èµ‹äºˆç›¸åŒæƒé‡ï¼Œè¿™æ ·åšæ²¡æœ‰åŒºåˆ†åº¦ï¼Œå¾€å¾€æ˜¯æ¨¡å‹æ€§èƒ½ä¸‹é™ã€‚

**æ¨¡å‹ç¤ºä¾‹**ï¼šå¯ä»¥ä¸Seq2Seqå¯¹æ¯”ï¼Œä¸­é—´å¢åŠ äº†ä¸€ä¸ªæƒé‡å±‚ï¼Œé€šè¿‡ä¸åŒçš„è¯­ä¹‰ç¼–ç å®ç°ï¼ˆä¹Ÿå°±æ˜¯ä¸åŒé•¿åº¦çš„å‘é‡ï¼‰

<img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806142115912-1682939089.png" alt="img" style="zoom:70%;" />

<img src="https://images2018.cnblogs.com/blog/1192699/201808/1192699-20180806142159178-1634092293.png" alt="img" style="zoom:70%;" />

å‚è€ƒèµ„æ–™ï¼šhttps://zhuanlan.zhihu.com/p/31547842ï¼ˆæ³¨æ„åŠ›æ¨¡å‹æ±‡æ€»æœºåŠå…¶åº”ç”¨ï¼‰ã€https://www.cnblogs.com/guoyaohua/p/9429924.htmlï¼ˆæ³¨æ„åŠ›æœºåˆ¶è§£é‡ŠåŠå…¶åº”ç”¨ï¼‰

### â˜£ï¸ Pointer Network



### â˜£ï¸ Graph Embedding



##è¿ç§»å­¦ä¹ 





## å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learningï¼‰

### â˜£ï¸ ä»‹ç»

![img](https://user-gold-cdn.xitu.io/2019/8/18/16ca41b0a58edcee?imageView2/0/w/1280/h/960/ignore-error/1)



å¦‚ä¸Šå›¾å·¦è¾¹æ‰€ç¤ºï¼Œä¸€ä¸ªagent(ä¾‹å¦‚ï¼šç©å®¶/æ™ºèƒ½ä½“ç­‰)åšå‡ºäº†ä¸€ä¸ªactionï¼Œå¯¹environmenté€ æˆäº†å½±å“ï¼Œä¹Ÿå°±æ˜¯æ”¹å˜äº†stateï¼Œè€Œenvironmentä¸ºäº†åé¦ˆç»™agentï¼Œagentå°±å¾—åˆ°äº†ä¸€ä¸ªå¥–åŠ±(ä¾‹å¦‚ï¼šç§¯åˆ†/åˆ†æ•°)ï¼Œä¸æ–­çš„è¿›è¡Œè¿™æ ·çš„å¾ªç¯ï¼Œç›´åˆ°ç»“æŸä¸ºæ­¢ã€‚

ä¸Šè¿°è¿‡ç¨‹å°±ç›¸å½“äºä¸€ä¸ª**é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹**

ä¸Šå›¾å³è¾¹æ‰€ç¤ºï¼ŒS0 çŠ¶æ€ç»è¿‡äº† a0 çš„è¡Œä¸ºåï¼Œè·å¾—äº†å¥–åŠ± r1 ï¼Œå˜æˆäº†çŠ¶æ€S1ï¼Œååˆç»è¿‡äº† a0 è¡Œä¸ºå¾—åˆ°å¥–åŠ± r2ï¼Œå˜æˆäº†çŠ¶æ€ S2 ï¼Œå¦‚æ­¤å¾€å¤å¾ªç¯ï¼Œç›´åˆ°ç»“æŸä¸ºæ­¢ã€‚

é€šè¿‡ä»¥ä¸Šçš„æè¿°ï¼Œå¤§å®¶éƒ½å·²ç»ç¡®å®šäº†ä¸€ä¸ªæ¦‚å¿µï¼Œä¹Ÿå°±æ˜¯agent(æ™ºèƒ½ä½“)åœ¨å½“ä¸‹åšå‡ºçš„å†³å®šè‚¯å®šä½¿å¾—æœªæ¥æ”¶ç›Šæœ€å¤§åŒ–ï¼Œé‚£ä¹ˆï¼Œä¸€ä¸ªé©¬å„¿å¯å¤«å†³ç­–è¿‡ç¨‹å¯¹åº”çš„å¥–åŠ±æ€»å’Œä¸ºï¼š

$$R=r_1+r_2+r_3+...+r_n$$

t æ—¶åˆ»(å½“ä¸‹)çš„æœªæ¥å¥–åŠ±ï¼Œåªè€ƒè™‘åé¢çš„å¥–åŠ±ï¼Œå‰é¢çš„æ”¹å˜ä¸äº†ï¼š

$R_t=r_t+r_{t+1}+r_{t+2}+...+r_n$

å½“å‰çš„è¡Œä¸ºå¯¹äºæœªæ¥æ˜¯ä¸ç¡®å®šæ€§çš„ï¼Œè¦æ‰“ä¸€ä¸ªæŠ˜æ‰£ï¼Œä¹Ÿå°±æ˜¯åŠ å…¥ä¸€ä¸ªç³»æ•°gammaï¼Œæ˜¯ä¸€ä¸ª 0 åˆ° 1 çš„å€¼ï¼š

$$R_t=r_1+\gamma_{}r_{t+1}+\gamma^2r_{t+2}+...+\gamma^{n-1}r_n$$

ç¦»å½“å‰è¶Šè¿œçš„æ—¶é—´ï¼Œgammaçš„æƒ©ç½šç³»æ•°å°±ä¼šè¶Šå¤§ï¼Œä¹Ÿå°±æ˜¯è¶Šä¸ç¡®å®šã€‚ä¸ºçš„å°±æ˜¯åœ¨å½“å‰å’Œæœªæ¥çš„å†³ç­–ä¸­å–å¾—ä¸€ä¸ªå¹³è¡¡ã€‚gammaå– 0 ï¼Œç›¸å½“äºä¸è€ƒè™‘æœªæ¥ï¼Œåªè€ƒè™‘å½“ä¸‹ï¼Œæ˜¯ä¸€ç§å¾ˆçŸ­è§†çš„åšæ³•ï¼›è€Œgammaå– 1 ï¼Œåˆ™å®Œå…¨è€ƒè™‘äº†æœªæ¥ï¼Œåˆæœ‰ç‚¹è¿‡è™‘äº†ã€‚æ‰€ä»¥ä¸€èˆ¬gammaä¼šå– 0 åˆ° 1 ä¹‹é—´çš„ä¸€ä¸ªå€¼ã€‚

Rt å¯ä»¥ç”¨ Rt+1 æ¥è¡¨ç¤ºï¼Œå†™æˆé€’æ¨å¼ï¼š

$$R_t=r_t+\gamma(r_{t+1}+\gamma(r_{t+2}+...))=r_t+\gamma_{}R_{t+1}$$

å‚è€ƒï¼šhttps://juejin.im/post/5d591d6af265da03c8151f4fã€https://easyai.tech/ai-definition/reinforcement-learning/

### â˜£ï¸ Q-Learning

**Q(s, a)å‡½æ•°(Quality)ï¼Œè´¨é‡å‡½æ•°ç”¨æ¥è¡¨ç¤ºæ™ºèƒ½ä½“åœ¨sçŠ¶æ€ä¸‹é‡‡ç”¨aåŠ¨ä½œå¹¶åœ¨ä¹‹åé‡‡å–æœ€ä¼˜åŠ¨ä½œæ¡ä»¶ä¸‹**çš„æ‰“æŠ˜çš„æœªæ¥å¥–åŠ±(å…ˆä¸ç®¡æœªæ¥çš„åŠ¨ä½œå¦‚ä½•é€‰æ‹©)ï¼š

$$Q(s_t,a_t)=maxR_{t+1}$$

å‡è®¾æœ‰äº†è¿™ä¸ªQå‡½æ•°ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±èƒ½å¤Ÿæ±‚å¾—åœ¨å½“å‰ t æ—¶åˆ»å½“ä¸­ï¼Œåšå‡ºå„ä¸ªå†³ç­–çš„æœ€å¤§æ”¶ç›Šå€¼ï¼Œé€šè¿‡å¯¹æ¯”è¿™äº›æ”¶ç›Šå€¼ï¼Œå°±èƒ½å¤Ÿ**å¾—åˆ° t æ—¶åˆ»æŸä¸ªå†³ç­–æ˜¯è¿™äº›å†³ç­–å½“ä¸­æ”¶ç›Šæœ€é«˜ã€‚**

$\pi(s)=argmax_aQ(s,a)$

äºæ˜¯ä¹ï¼Œæ ¹æ®Qå‡½æ•°çš„é€’æ¨å…¬å¼å¯ä»¥å¾—åˆ°ï¼š

$$Q(s_t,a_t)=r+\gamma_{}max_aQ(s_{t+1},a_{t+1})$$

**è¿™å°±æ˜¯æ³¨æ˜çš„è´å°”æ›¼å…¬å¼ã€‚**è´å°”æ›¼å…¬å¼å®é™…éå¸¸åˆç†ã€‚å¯¹äºæŸä¸ªçŠ¶æ€æ¥è®²ï¼Œæœ€å¤§åŒ–æœªæ¥å¥–åŠ±ç›¸å½“äºæœ€å¤§åŒ–å³åˆ»å¥–åŠ±ä¸ä¸‹ä¸€çŠ¶æ€æœ€å¤§æœªæ¥å¥–åŠ±ä¹‹å’Œã€‚

**Q-learningçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡è´å°”æ›¼å…¬å¼è¿­ä»£åœ°è¿‘ä¼¼Q-å‡½æ•°ã€‚


å‚è€ƒï¼šhttps://juejin.im/post/5d591d6af265da03c8151f4f

### â˜£ï¸ DQN

Deep Q Learning(DQN)æ˜¯ä¸€ç§èåˆäº†ç¥ç»ç½‘ç»œå’Œçš„Q-Learningæ–¹æ³•ã€‚

**ç¥ç»ç½‘ç»œçš„ä½œç”¨**



![img](https://user-gold-cdn.xitu.io/2019/8/18/16ca41b0dc74ff8e?imageView2/0/w/1280/h/960/ignore-error/1)



ä½¿ç”¨è¡¨æ ¼æ¥å­˜å‚¨æ¯ä¸€ä¸ªçŠ¶æ€ state, å’Œåœ¨è¿™ä¸ª state æ¯ä¸ªè¡Œä¸º action æ‰€æ‹¥æœ‰çš„ Q å€¼. è€Œå½“ä»Šé—®é¢˜æ˜¯åœ¨å¤ªå¤æ‚, çŠ¶æ€å¯ä»¥å¤šåˆ°æ¯”å¤©ä¸Šçš„æ˜Ÿæ˜Ÿè¿˜å¤š(æ¯”å¦‚ä¸‹å›´æ£‹). å¦‚æœå…¨ç”¨è¡¨æ ¼æ¥å­˜å‚¨å®ƒä»¬, ææ€•æˆ‘ä»¬çš„è®¡ç®—æœºæœ‰å†å¤§çš„å†…å­˜éƒ½ä¸å¤Ÿ, è€Œä¸”æ¯æ¬¡åœ¨è¿™ä¹ˆå¤§çš„è¡¨æ ¼ä¸­æœç´¢å¯¹åº”çš„çŠ¶æ€ä¹Ÿæ˜¯ä¸€ä»¶å¾ˆè€—æ—¶çš„äº‹. ä¸è¿‡, åœ¨æœºå™¨å­¦ä¹ ä¸­, æœ‰ä¸€ç§æ–¹æ³•å¯¹è¿™ç§äº‹æƒ…å¾ˆåœ¨è¡Œ, é‚£å°±æ˜¯ç¥ç»ç½‘ç»œ.

æˆ‘ä»¬å¯ä»¥å°†çŠ¶æ€å’ŒåŠ¨ä½œå½“æˆç¥ç»ç½‘ç»œçš„è¾“å…¥, ç„¶åç»è¿‡ç¥ç»ç½‘ç»œåˆ†æåå¾—åˆ°åŠ¨ä½œçš„ Q å€¼, è¿™æ ·æˆ‘ä»¬å°±æ²¡å¿…è¦åœ¨è¡¨æ ¼ä¸­è®°å½• Q å€¼, è€Œæ˜¯ç›´æ¥ä½¿ç”¨ç¥ç»ç½‘ç»œç”Ÿæˆ Q å€¼.

è¿˜æœ‰ä¸€ç§å½¢å¼çš„æ˜¯è¿™æ ·, æˆ‘ä»¬ä¹Ÿèƒ½åªè¾“å…¥çŠ¶æ€å€¼, è¾“å‡ºæ‰€æœ‰çš„åŠ¨ä½œå€¼, ç„¶åæŒ‰ç…§ Q learning çš„åŸåˆ™, ç›´æ¥é€‰æ‹©æ‹¥æœ‰æœ€å¤§å€¼çš„åŠ¨ä½œå½“åšä¸‹ä¸€æ­¥è¦åšçš„åŠ¨ä½œ.

æˆ‘ä»¬å¯ä»¥æƒ³è±¡, ç¥ç»ç½‘ç»œæ¥å—å¤–éƒ¨çš„ä¿¡æ¯, ç›¸å½“äºçœ¼ç›é¼»å­è€³æœµæ”¶é›†ä¿¡æ¯, ç„¶åé€šè¿‡å¤§è„‘åŠ å·¥è¾“å‡ºæ¯ç§åŠ¨ä½œçš„å€¼, æœ€åé€šè¿‡å¼ºåŒ–å­¦ä¹ çš„æ–¹å¼é€‰æ‹©åŠ¨ä½œ.

**ç¥ç»ç½‘ç»œè®¡ç®—Qå€¼**

è¿™ä¸€éƒ¨åˆ†å°±è·Ÿç›‘ç£å­¦ä¹ çš„ç¥ç»ç½‘ç»œä¸€æ ·äº†æˆ‘ï¼Œè¾“å…¥çŠ¶æ€å€¼ï¼Œè¾“å‡ºä¸ºQå€¼ï¼Œæ ¹æ®å¤§é‡çš„æ•°æ®å»è®­ç»ƒç¥ç»ç½‘ç»œçš„å‚æ•°ï¼Œæœ€ç»ˆå¾—åˆ°Q-Learningçš„è®¡ç®—æ¨¡å‹ï¼Œè¿™æ—¶å€™æˆ‘ä»¬å°±å¯ä»¥åˆ©ç”¨è¿™ä¸ªæ¨¡å‹æ¥è¿›è¡Œå¼ºåŒ–å­¦ä¹ äº†ã€‚



![img](https://user-gold-cdn.xitu.io/2019/8/18/16ca41b1338135c6?imageView2/0/w/1280/h/960/ignore-error/1)


å‚è€ƒï¼šhttps://juejin.im/post/5d591d6af265da03c8151f4f

## æ¨èç³»ç»Ÿ

### âœ… ååŒè¿‡æ»¤æ¨¡å‹

**ç›¸ä¼¼æ€§åº¦é‡æ–¹æ³•** 

1.æ¬§å‡ é‡Œå¾—è·ç¦»ï¼šè¯¥è·ç¦»åªæœ‰å½“ä¸¤è€…ç‰¹å¾å‘é‡ä¸­æ¯ä¸ªç‰¹å¾éƒ½è¾ƒå°æ—¶ï¼Œç‰¹å¾å‘é‡é—´è·ç¦»æ‰æ¯”è¾ƒå°ã€‚ 

2.çš®å°”æ£®ç›¸å…³ç³»æ•°ï¼šè¯¥ç³»æ•°åº¦é‡ä¸¤ä¸ªç‰¹å¾å‘é‡ä¹‹é—´çš„çº¿æ€§ç›¸å…³æ€§ï¼ŒäºŒè€…çº¿æ€§ç›¸å…³ç¨‹åº¦è¶Šå¼ºï¼Œè¯¥åº¦é‡å€¼è¶Šé«˜ã€‚è¯¥åº¦é‡é€‚ç”¨äºä¸åŒç‰¹å¾çš„èŒƒå›´ä¸ä¸€æ ·çš„æƒ…å†µã€‚

**åŸºäºç”¨æˆ·ç›¸ä¼¼åº¦çš„æ¨è** 

ä¸ºç”¨æˆ·Aæä¾›æ¨èï¼Œé¦–å…ˆé€‰æ‹©çš®å°”æ£®ç›¸å…³ç³»æ•°è¿›è¡Œåº¦é‡ï¼Œè®¡ç®—ä¸Aæœ€ç›¸ä¼¼çš„Nä¸ªäººï¼Œç„¶åè®¡ç®—æ¯éƒ¨ç”µå½±åœ¨ç›¸ä¼¼åº¦åŠ æƒå¹³å‡ä¸‹çš„åŠ æƒå¹³å‡å€¼ï¼Œä»¥è¯¥åˆ†å€¼ä½œä¸ºæ¯éƒ¨ç”µå½±çš„æ¨èå€¼ã€‚

**åŸºäºç‰©å“çš„åä½œå‹è¿‡æ»¤** 

é¦–å…ˆå°†åŸå§‹å­—å…¸çš„é”®å’Œå€¼è¿›è¡Œè°ƒæ¢ï¼Œæ­¤æ—¶é”®ä¸ºç‰©å“ï¼Œå€¼ä¸ºä¸€ä¸ªå­—å…¸ï¼Œå­—å…¸çš„é”®ä¸ºç”¨æˆ·ï¼Œå€¼ä¸ºåˆ†å€¼ã€‚æ ¹æ®æ­¤å­—å…¸è¿è¡Œä¸Šè¿°çš„ç®—æ³•ï¼Œå…ˆæ±‚ç‰©å“ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚

æ¯”å¦‚è¦ç»™ç”¨æˆ·Aæä¾›æ¨èï¼ŒAåšå‡ºè¯„ä»·çš„ç‰©å“æœ‰1ï¼Œ2ï¼Œ3.æœªä½œå‡ºè¯„ä»·çš„ç‰©å“æœ‰4ï¼Œ5ï¼Œ6ã€‚è®¡ç®—ç‰©å“4çš„åˆ†å€¼ï¼Œéœ€è¦å…ˆè®¡ç®—ç‰©å“4ä¸ç‰©å“1ï¼Œ2ï¼Œ3çš„ç›¸ä¼¼åº¦ï¼Œä»¥ç›¸ä¼¼åº¦ä½œä¸ºæƒå€¼ï¼Œå¯¹ç‰©å“1ï¼Œ2ï¼Œ3çš„è¯„åˆ†è¿›è¡ŒåŠ æƒå¹³å‡ï¼Œä»¥æ­¤ä½œä¸ºå¯¹ç‰©å“4 è¯„åˆ†ã€‚

```python
# -*- coding: utf-8 -*-
__author__ = 'Bai Chenjia'
from math import *


class recommendation:
    def __init__(self):
        self.critics = {'Lisa Rose': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5, 'Just My Luck': 3.0,
                                      'Superman Returns': 3.5, 'You, Me and Dupree': 2.5, 'The Night Listener': 3.0},
                        'Gene Seymour': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5, 'Just My Luck': 1.5,
                                         'Superman Returns': 5.0, 'The Night Listener': 3.0, 'You, Me and Dupree': 3.5},
                        'Michael Phillips': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,
                                             'Superman Returns': 3.5, 'The Night Listener': 4.0},
                        'Claudia Puig': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0, 'The Night Listener': 4.5,
                                         'Superman Returns': 4.0, 'You, Me and Dupree': 2.5},
                        'Mick LaSalle': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, 'Just My Luck': 2.0,
                                         'Superman Returns': 3.0, 'The Night Listener': 3.0, 'You, Me and Dupree': 2.0},
                        'Jack Matthews': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, 'The Night Listener': 3.0,
                                          'Superman Returns': 5.0, 'You, Me and Dupree': 3.5},
                        'Toby': {'Snakes on a Plane': 4.5, 'You, Me and Dupree': 1.0, 'Superman Returns': 4.0}}

    def sim_distance(self, person1, person2, new_critics=0):
        """
        ä½¿ç”¨æ¬§æ°è·ç¦»è®¡ç®—ç›¸ä¼¼åº¦. è¿”å›ç›¸ä¼¼åº¦æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°å€¼. è¿”å›å€¼ä¸º0ä»£è¡¨æ²¡æœ‰ç›¸ä¼¼æ€§.
        é¦–å…ˆè®¡ç®—ä¸¤ä¸ªäººæ‰€çœ‹ç›¸åŒç”µå½±è¯„åˆ†çš„æ¬§æ°è·ç¦»L. å¦‚ä¸¤äººæ²¡æœ‰è§‚çœ‹ç›¸åŒç”µå½±åˆ™è¿”å›0,å¦åˆ™è®¡ç®—ç›¸ä¼¼åº¦å…¬å¼ä¸º 1/(L+1).
        å…¶ä¸­åˆ†æ¯ä½¿ç”¨L+1æ˜¯ä¸ºäº†é˜²æ­¢å‡ºç°åˆ†æ¯ä¸º0çš„æƒ…å†µ. æ¬§æ°è·ç¦»è¶Šå¤§ï¼Œç›¸ä¼¼åº¦è¶Šå°;æ¬§æ°è·ç¦»è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šå¤§
        """
        if new_critics != 0:
            new_critics = self.transformPrefs()
            movielist1 = new_critics[person1]
            movielist2 = new_critics[person2]   # dict
            commonlist = []        # list
            for item in movielist1:
                if item in movielist2:
                    commonlist.append(item)
            if len(commonlist) == 0:   # æ²¡æœ‰å…¬å…±å…ƒç´ ï¼Œç›¸ä¼¼åº¦ä¸º0
                return 0
            # è®¡ç®—æ¬§æ°è·ç¦»(ä½¿ç”¨åˆ—è¡¨ç”Ÿæˆå¼)
            distance = sum([pow(movielist1[item] - movielist2[item], 2) for item in commonlist])
            sim = 1 / (sqrt(distance) + 1)
            return sim
        else:
            movielist1 = self.critics[person1]
            movielist2 = self.critics[person2]   # dict
            commonlist = []        # list
            for item in movielist1:
                if item in movielist2:
                    commonlist.append(item)
            if len(commonlist) == 0:   # æ²¡æœ‰å…¬å…±å…ƒç´ ï¼Œç›¸ä¼¼åº¦ä¸º0
                return 0
            # è®¡ç®—æ¬§æ°è·ç¦»(ä½¿ç”¨åˆ—è¡¨ç”Ÿæˆå¼)
            distance = sum([pow(movielist1[item] - movielist2[item], 2) for item in commonlist])
            sim = 1 / (sqrt(distance) + 1)
            return sim

    def sim_pearson(self, person1, person2):
        """
        è®¡ç®—çš®å°”æ£®ç›¸å…³ç³»æ•°.   åº¦é‡ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„çº¿æ€§ç›¸å…³æ€§.
        å¦‚æœä¸€ä¸ªäººæ€»æ˜¯ç»™å‡ºæ¯”å¦ä¸€ä¸ªäººæ›´é«˜çš„åˆ†æ”¯ï¼Œä½†äºŒè€…çš„åˆ†å€¼åªå·®åŸºæœ¬ä¿æŒä¸€è‡´ï¼Œåˆ™ä»–ä»¬ä»ç„¶å­˜åœ¨å¾ˆå¥½çš„ç›¸å…³æ€§.
        :return:ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„çš®å°”æ£®ç›¸å…³ç³»æ•°ï¼Œè¿”å›æµ®ç‚¹æ•°
        """
        movielist1 = self.critics[person1]
        movielist2 = self.critics[person2]
        commonlist = []
        for item in movielist1:
            if item in movielist2:
                commonlist.append(item)  # key
        if len(commonlist) == 0:  # æ²¡æœ‰å…±åŒé¡¹
            return 0

        sum1 = sum([self.critics[person1][item] for item in commonlist])   # person1è¯„åˆ†å’Œ
        sum2 = sum([self.critics[person2][item] for item in commonlist])   # person2è¯„åˆ†å’Œ

        sum1sq = sum([pow(self.critics[person1][item], 2) for item in commonlist])  # person1è¯„åˆ†å¹³æ–¹å’Œ
        sum2sq = sum([pow(self.critics[person2][item], 2) for item in commonlist])  # person2è¯„åˆ†å¹³æ–¹å’Œ

        psum = sum([self.critics[person1][item] * self.critics[person2][item]
                    for item in commonlist])  # äº¤å‰é¡¹ä¹˜ç§¯å’Œ

        # calculate perarson
        n = len(commonlist)
        num = (psum - (sum1 * sum2 / n))
        den = sqrt((sum1sq - pow(sum1, 2) / n) * (sum2sq - pow(sum2, 2) / n))
        if den == 0:
            return 0
        r = num / den
        return r

    def topMatches(self, person, n=5, similarity=sim_pearson):
        """
        è¿”å›ä¸personæœ€ç›¸ä¼¼çš„nä¸ªäººçš„å§“åå’Œç›¸ä¼¼åº¦. ç›¸ä¼¼åº¦åº¦é‡é€‰ç”¨ çš®å°”æ£®ç›¸å…³ç³»æ•°
        :param person: è¦è®¡ç®—çš„äººçš„å§“å
        :param n: ä¸personç›¸ä¼¼åº¦æœ€é«˜çš„nä¸ªäºº
        :param similarity: ç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•
        :return: è¿”å›ä¸€ä¸ªlistï¼Œlistä¸­å…ƒç»„å­˜å‚¨çš„å½¢å¼ä¸ºï¼ˆç›¸å…³æ€§, å§“åï¼‰
        """
        if person not in self.critics:
            print "person input error!"
            return 0
        scores = [(similarity(self, person, item), item) for item in self.critics if item != person]
        # print scores[:]
        return sorted(scores, key=lambda it: -it[0])[0:n]  # å–ç›¸ä¼¼åº¦æœ€é«˜çš„nä½

    def getRecommendations(self, person, similarity=sim_pearson):
        """
        ç”¨ä¸personçš„ç›¸ä¼¼åº¦å¯¹å…¶ä»–äººå¯¹personæœªçœ‹è¿‡çš„å½±ç‰‡è¿›è¡ŒåŠ æƒå¹³å‡ï¼Œå¯¹personæœªçœ‹å½±ç‰‡è¿›è¡ŒåŠ æƒæ‰“åˆ†
        :param person:
        :param similarity: ç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•
        :return: è¿”å›å¯¹ person æœªçœ‹å½±ç‰‡çš„æ¨èè¯„åˆ†ï¼ŒæŒ‰è¯„åˆ†æ’åºè¿›è¡Œæ¨è
        """
        moviescore_dict = {}  # å½±ç‰‡--å½±ç‰‡å¯¹åº”çš„è¯„åˆ†å’Œ
        moviesim_dict = {}  # å½±ç‰‡--å½±ç‰‡å¯¹åº”çš„ç›¸ä¼¼åº¦å’Œ
        for other in self.critics:
            sim = similarity(self, other, person)  # äººä¹‹é—´çš„ç›¸ä¼¼åº¦
            if other == person:
                continue
            if sim < 0:
                continue
            for item in self.critics[other]:
                if item not in self.critics[person]:  # å¦‚æœpersonæœªå¯¹è¯¥ç”µå½±è¿›è¡Œè¯„ä»·
                    moviescore_dict.setdefault(item, 0)
                    moviescore_dict[item] += self.critics[other][item] * sim

                    moviesim_dict.setdefault(item, 0)
                    moviesim_dict[item] += sim

        result = [(float(moviescore_dict[item] / moviesim_dict[item]), item)
                  for item in moviescore_dict]
        result = sorted(result, key=lambda e1: -e1[0])
        return result

    def transformPrefs(self):
        """
        å°†self.criticsä¸­çš„ äºº--ç”µå½± çš„é”®å’Œå€¼è¿›è¡Œè°ƒæ¢ï¼Œåå‘æ„é€ å­—å…¸
        :return: é”®å€¼è°ƒæ¢åçš„ dict
        """
        result = {}
        for person in self.critics:
            for item in self.critics[person]:
                result.setdefault(item, {})
                result[item][person] = self.critics[person][item]
        return result

    def calculateSimilarItems(self, similarity=sim_distance):
        """
        åˆ©ç”¨åå‘æ„é€ çš„ ç‰©å“--äºº  å­—å…¸ï¼Œæ„é€ ä¸€ä¸ªç‰©å“ç›¸ä¼¼åº¦å­—å…¸
        å­—å…¸çš„é”®æ˜¯ç‰©å“ï¼Œå€¼æ˜¯ä¸è¯¥ç‰©å“æœ€ç›¸ä¼¼çš„nä¸ªç‰©å“ä»¥åŠç‰©å“ä¹‹é—´çš„ç›¸ä¼¼åº¦
        :param n: ä¸ç‰©å“æœ€ç›¸ä¼¼çš„nä¸ªç‰©å“
        :return: dict
        """
        result = {}
        reverse_critics = self.transformPrefs()
        for movie in reverse_critics:
            scores = [(similarity(self, movie, item, 100), item)
                      for item in reverse_critics if item != movie]
            scores = sorted(scores, key=lambda it: -it[0])       # å–ç›¸ä¼¼åº¦æœ€é«˜çš„nä½
            result[movie] = scores
        return result

    def getRecommendedItems(self, user):
        """
        æŒ‡å®šä¸€ä¸ªåå­—userï¼Œè®¡ç®—å¯¹è¯¥ç”¨æˆ·çš„ç”µå½±æ¨è
        é¦–å…ˆåˆ©ç”¨ calculateSimilarItemsæ–¹æ³•è®¡ç®—ç‰©å“ä¹‹é—´çš„ç›¸ä¼¼åº¦dict
        æ ¹æ®è¯¥ç”¨æˆ·å¯¹æ›¾ç»çœ‹è¿‡çš„å½±ç‰‡çš„è¯„åˆ†ä»¥åŠè¯¥ç”µå½±ä¸æœªçœ‹è¿‡ç”µå½±çš„ç›¸ä¼¼åº¦ä¼°è®¡ç”¨æˆ·å¯¹æœªçœ‹ç”µå½±çš„è¯„åˆ†ï¼Œæ ¹æ®è¯„åˆ†è¿›è¡Œæ¨è
        :param user: æŒ‡å®šçš„äºº
        :return:
        """
        userRatings = self.critics[user]  # è¿”å›è¯¥ç”¨æˆ·æ‰€çœ‹ç”µå½±å’Œè¯„åˆ†
        itemMatch = self.calculateSimilarItems()  # è¿”å›ç‰©å“çš„ç›¸ä¼¼åº¦å­—å…¸
        scores = {}
        totalsim = {}
        for item, rating in userRatings.items():
            for similarity, item2 in itemMatch[item]:
                if item2 in userRatings:   # è‹¥è¯¥ç”µå½±è¢«ç”¨æˆ·è¯„åˆ†
                    continue
                scores.setdefault(item2, 0)
                scores[item2] += similarity * rating
                totalsim.setdefault(item2, 0)
                totalsim[item2] += similarity
        ranking = [(score / totalsim[item], item) for item, score in scores.items()]
        ranking = sorted(ranking, key=lambda it: -it[0])
        return ranking


if __name__ == '__main__':
    system = recommendation()

    # 1.æ¬§å‡ é‡Œå¾—è·ç¦»è®¡ç®—ç›¸ä¼¼åº¦
    print "\næ¬§å‡ é‡Œå¾—ç›¸ä¼¼åº¦:", system.sim_distance('Lisa Rose', 'Gene Seymour')

    # 2.çš®å°”æ£®ç›¸å…³ç³»æ•°è®¡ç®—
    print "\nçš®å°”æ£®ç›¸å…³ç³»æ•°:", system.sim_pearson('Lisa Rose', 'Gene Seymour')

    # 3.è®¡ç®—ä¸æŸäººç›¸ä¼¼åº¦æœ€é«˜çš„nä¸ªäºº
    print "\nä¸ Toby ç›¸ä¼¼åº¦æœ€é«˜çš„nä¸ªäºº", system.topMatches('Toby', n=3)[:]

    # 4.è¾“å…¥äººåï¼Œå¯¹è¯¥äººæœªè§‚çœ‹çš„å½±ç‰‡æ¨è
    print "\nç»™ Toby æœªè§‚çœ‹å½±ç‰‡çš„æ¨èæ˜¯", system.getRecommendations('Toby')[:]

    # 5.æ„å»ºç‰©å“ç›¸ä¼¼åº¦å­—å…¸
    print "\nç”µå½±ä¹‹é—´çš„ç›¸ä¼¼åº¦å­—å…¸æ˜¯ï¼š"
    for key, value in system.calculateSimilarItems().iteritems():
        print key, ":", value[:]

    # 6.æ ¹æ®ç‰©å“ç›¸ä¼¼åº¦ï¼Œè¾“å…¥äººåï¼Œæ¨èå½±ç‰‡
    print "\næ ¹æ®ç”µå½±ç›¸ä¼¼åº¦ç»™ Toby æ¨èçš„å½±ç‰‡æ˜¯ï¼š", system.getRecommendedItems('Toby')[:]
```

å‚è€ƒèµ„æ–™ï¼šhttps://blog.csdn.net/bcj296050240/article/details/50810662

## å›å½’æ¨¡å‹

### â˜£ï¸ çº¿æ€§è§„åˆ’ï¼ˆLinear Regressionï¼‰



### âœ… é€»è¾‘å›å½’ï¼ˆLogistic Regressionï¼‰

![img](https://yuanxiaosc.github.io/2018/06/21/%E6%94%B9%E8%BF%9B%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E2%80%94%E2%80%94%E4%BA%A4%E5%8F%89%E7%86%B5/one_1.png)



### âœ… SoftMax

<img src="https://pic1.zhimg.com/v2-11758fbc2fc5bbbc60106926625b3a4f_1200x500.jpg" style="zoom:70%">

```python
scores = np.array([123, 456, 789])    # example with 3 classes and each having large scores
scores -= np.max(scores)    # scores becomes [-666, -333, 0]
p = np.exp(scores) / np.sum(np.exp(scores))
```

ä¸»è¦ç”¨äºå¤šåˆ†ç±»å›å½’ï¼Œå¯ä»¥å¯¹å‘é‡è¿›è¡Œå¤„ç†ï¼Œæœ€åå¾—å‡ºçš„æ¦‚ç‡æ€»å’Œæ˜¯1

å‚è€ƒèµ„æ–™ï¼šhttps://zhuanlan.zhihu.com/p/25723112ï¼ˆsoftmaxåº”ç”¨ï¼‰

### â˜£ï¸ ä¼¼ç„¶å‡½æ•°ï¼ˆLikelihood Functionï¼‰



### â˜£ï¸ è´å¶æ–¯ç½‘ç»œï¼ˆBayes Networkï¼‰





å‚è€ƒèµ„æ–™ï¼šhttps://cloud.tencent.com/developer/article/1102103ï¼ˆåˆ†ç±»ç®—æ³•æ€»ç»“ï¼‰

## ç»Ÿè®¡æ¨¡å‹-è’™åœˆ

### â˜£ï¸ æ¡ä»¶éšæœºåœºï¼ˆConditional Random Fieldï¼‰



### âœ… é©¬å°”å¯å¤«æ¨¡å‹ï¼ˆMarkov Modelsï¼‰

**æ¦‚è¿°**ï¼šé©¬å°”å¯å¤«è¿‡ç¨‹ä¸­ï¼Œä»»ä½•ä¸€ä¸ªçŠ¶æ€ï¼Œåªä¸å‰ä¸€ä¸ªçŠ¶æ€ç›¸å…³

<img src="https://pic4.zhimg.com/80/648a55725e67d718d97d6a475891d70b_hd.jpg" style="width:50%"/>

```python
# å…³ç³»çŸ©é˜µ
transfer_matrix = np.array([[0.6,0.2,0.2],[0.3,0.4,0.3],[0,0.3,0.7]],dtype='float32') 
# 
start_matrix = np.array([[0.5,0.3,0.2]],dtype='float32') 

value1 = []
value2 = []
value3 = []
for i in range(30):
    start_matrix = np.dot(start_matrix,transfer_matrix)
    value1.append(start_matrix[0][0])
    value2.append(start_matrix[0][1])
    value3.append(start_matrix[0][2])
print(start_matrix)

# è¾“å‡º[[0.23076934 0.30769244 0.4615386 ]]
```

### ğŸ’¤ éšé©¬å°”å¯å¤«æ¨¡å‹ï¼ˆHidden Markov Modelsï¼‰

#### âœ… å‰è¨€

éšé©¬å°”å¯å¤«æ¨¡å‹å³é©¬å°”å¯å¤«é“¾ä¸ŠåŠ äº†ä¸€å±‚éšæœºè¿‡ç¨‹ï¼Œä¸€èˆ¬é‡‡ç”¨Baum-Welchç®—æ³•å’ŒViterbiç®—æ³•è¿›è¡Œæ±‚è§£

![preview](https://pic2.zhimg.com/792e033ff9b0418b3b6c9bbaef30fd83_r.jpg)

#### ğŸ†˜ Baum-Welchç®—æ³•

ä¸€ç§EMç®—æ³•

#### ğŸ†˜ Viterbiç®—æ³•



#### ğŸ†˜ æºç -Baum-Welchç®—æ³•

```python
"""
éšé©¬å°”ç§‘å¤«æ¨¡å‹
ä¸‰ç±»é—®é¢˜ï¼š1.æ¦‚ç‡è®¡ç®— 2.å­¦ä¹ é—®é¢˜ï¼ˆå‚æ•°ä¼°è®¡ï¼‰ 3.é¢„æµ‹é—®é¢˜ï¼ˆçŠ¶æ€åºåˆ—çš„é¢„æµ‹ï¼‰
"""
import numpy as np
from itertools import accumulate

class GenData:
    """
    æ ¹æ®éšé©¬å°”ç§‘å¤«æ¨¡å‹ç”Ÿæˆç›¸åº”çš„è§‚æµ‹æ•°æ®
    """
    def __init__(self, hmm, n_sample):
        self.hmm = hmm
        self.n_sample = n_sample

    def _locate(self, prob_arr):
        # ç»™å®šæ¦‚ç‡å‘é‡ï¼Œè¿”å›çŠ¶æ€
        seed = np.random.rand(1)
        for state, cdf in enumerate(accumulate(prob_arr)):
            if seed <= cdf:
                return state
        return

    def init_state(self):
        # æ ¹æ®åˆå§‹çŠ¶æ€æ¦‚ç‡å‘é‡ï¼Œç”Ÿæˆåˆå§‹çŠ¶æ€
        return self._locate(self.hmm.S)

    def state_trans(self, current_state):
        # è½¬ç§»çŠ¶æ€
        return self._locate(self.hmm.A[current_state])

    def gen_obs(self, current_state):
        # ç”Ÿæˆè§‚æµ‹
        return self._locate(self.hmm.B[current_state])

    def gen_data(self):
        # æ ¹æ®æ¨¡å‹äº§ç”Ÿè§‚æµ‹æ•°æ®
        current_state = self.init_state()
        start_obs = self.gen_obs(current_state)
        state = [current_state]
        obs = [start_obs]
        n = 0
        while n < self.n_sample - 1:
            n += 1
            current_state = self.state_trans(current_state)
            state.append(current_state)
            obs.append(self.gen_obs(current_state))
        return state, obs


class HMM:
    def __init__(self, n_state, n_obs, S=None, A=None, B=None):
        self.n_state = n_state  # çŠ¶æ€çš„ä¸ªæ•°n
        self.n_obs = n_obs  # è§‚æµ‹çš„ç§ç±»æ•°m
        self.S = S  # 1*n, åˆå§‹çŠ¶æ€æ¦‚ç‡å‘é‡
        self.A = A  # n*n, çŠ¶æ€è½¬ç§»æ¦‚ç‡çŸ©é˜µ
        self.B = B  # n*m, è§‚æµ‹ç”Ÿæˆæ¦‚ç‡çŸ©é˜µ

def _alpha(hmm, obs, t):
    # è®¡ç®—æ—¶åˆ»tå„ä¸ªçŠ¶æ€çš„å‰å‘æ¦‚ç‡
    b = hmm.B[:, obs[0]]
    alpha = np.array([hmm.S * b])  # n*1
    for i in range(1, t + 1):
        alpha = (alpha @ hmm.A) * np.array([hmm.B[:, obs[i]]])
    return alpha[0]

def forward_prob(hmm, obs):
    # å‰å‘ç®—æ³•è®¡ç®—æœ€ç»ˆç”Ÿæˆè§‚æµ‹åºåˆ—çš„æ¦‚ç‡, å³å„ä¸ªçŠ¶æ€ä¸‹æ¦‚ç‡ä¹‹å’Œ
    alpha = _alpha(hmm, obs, len(obs) - 1)
    return np.sum(alpha)

def _beta(hmm, obs, t):
    # è®¡ç®—æ—¶åˆ»tå„ä¸ªçŠ¶æ€çš„åå‘æ¦‚ç‡
    beta = np.ones(hmm.n_state)
    for i in reversed(range(t + 1, len(obs))):
        beta = np.sum(hmm.A * hmm.B[:, obs[i]] * beta, axis=1)
    return beta

def backward_prob(hmm, obs):
    # åå‘ç®—æ³•è®¡ç®—ç”Ÿæˆè§‚æµ‹åºåˆ—çš„æ¦‚ç‡
    beta = _beta(hmm, obs, 0)
    return np.sum(hmm.S * hmm.B[:, obs[0]] * beta)

def fb_prob(hmm, obs, t=None):
    # å°†å‰å‘å’Œåå‘åˆå¹¶
    if t is None:
        t = 0
    res = _alpha(hmm, obs, t) * _beta(hmm, obs, t)
    return res.sum()

def _gamma(hmm, obs, t):
    # è®¡ç®—æ—¶åˆ»tå¤„äºå„ä¸ªçŠ¶æ€çš„æ¦‚ç‡
    alpha = _alpha(hmm, obs, t)
    beta = _beta(hmm, obs, t)
    prob = alpha * beta
    return prob / prob.sum()


def point_prob(hmm, obs, t, i):
    # è®¡ç®—æ—¶åˆ»tå¤„äºçŠ¶æ€içš„æ¦‚ç‡
    prob = _gamma(hmm, obs, t)
    return prob[i]

def _xi(hmm, obs, t):
    alpha = np.mat(_alpha(hmm, obs, t))
    beta_p = _beta(hmm, obs, t + 1)
    obs_prob = hmm.B[:, obs[t + 1]]
    obs_beta = np.mat(obs_prob * beta_p)
    alpha_obs_beta = np.asarray(alpha.T * obs_beta)
    xi = alpha_obs_beta * hmm.A
    return xi / xi.sum()


def fit(hmm, obs_data, maxstep=100):
    # åˆ©ç”¨Baum-Welchç®—æ³•å­¦ä¹ 
    hmm.A = np.ones((hmm.n_state, hmm.n_state)) / hmm.n_state
    hmm.B = np.ones((hmm.n_state, hmm.n_obs)) / hmm.n_obs
    hmm.S = np.random.sample(hmm.n_state)  # åˆå§‹çŠ¶æ€æ¦‚ç‡çŸ©é˜µï¼ˆå‘é‡ï¼‰ï¼Œçš„åˆå§‹åŒ–å¿…é¡»éšæœºçŠ¶æ€ï¼Œå¦åˆ™å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜
    hmm.S = hmm.S / hmm.S.sum()
    step = 0
    while step < maxstep:
        xi = np.zeros_like(hmm.A)
        gamma = np.zeros_like(hmm.S)
        B = np.zeros_like(hmm.B)
        S = _gamma(hmm, obs_data, 0)
        for t in range(len(obs_data) - 1):
            tmp_gamma = _gamma(hmm, obs_data, t)
            gamma += tmp_gamma
            xi += _xi(hmm, obs_data, t)
            B[:, obs_data[t]] += tmp_gamma

        # æ›´æ–° A
        for i in range(hmm.n_state):
            hmm.A[i] = xi[i] / gamma[i]
        # æ›´æ–° B
        tmp_gamma_end = _gamma(hmm, obs_data, len(obs_data) - 1)
        gamma += tmp_gamma_end
        B[:, obs_data[-1]] += tmp_gamma_end
        for i in range(hmm.n_state):
            hmm.B[i] = B[i] / gamma[i]
        # æ›´æ–° S
        hmm.S = S
        step += 1
    return hmm

def predict(hmm, obs):
    # é‡‡ç”¨Viterbiç®—æ³•é¢„æµ‹çŠ¶æ€åºåˆ—
    N = len(obs)
    nodes_graph = np.zeros((hmm.n_state, N), dtype=int)  # å­˜å‚¨æ—¶åˆ»tä¸”çŠ¶æ€ä¸ºiæ—¶ï¼Œ å‰ä¸€ä¸ªæ—¶åˆ»t-1çš„çŠ¶æ€ï¼Œç”¨äºæ„å»ºæœ€ç»ˆçš„çŠ¶æ€åºåˆ—
    delta = hmm.S * hmm.B[:, obs[0]]  # å­˜å‚¨åˆ°tæ—¶åˆ»ï¼Œä¸”æ­¤åˆ»çŠ¶æ€ä¸ºiçš„æœ€å¤§æ¦‚ç‡
    nodes_graph[:, 0] = range(hmm.n_state)

    for t in range(1, N):
        new_delta = []
        for i in range(hmm.n_state):
            temp = [hmm.A[j, i] * d for j, d in enumerate(delta)]  # å½“å‰çŠ¶æ€ä¸ºiæ—¶ï¼Œ é€‰å–æœ€ä¼˜çš„å‰ä¸€æ—¶åˆ»çŠ¶æ€
            max_d = max(temp)
            new_delta.append(max_d * hmm.B[i, obs[t]])
            nodes_graph[i, t] = temp.index(max_d)
        delta = new_delta

    current_state = np.argmax(nodes_graph[:, -1])
    path = []
    t = N
    while t > 0:
        path.append(current_state)
        current_state = nodes_graph[current_state, t - 1]
        t -= 1
    return list(reversed(path))


if __name__ == '__main__':
    # S = np.array([0.2, 0.4, 0.4])
    # A = np.array([[0.5, 0.2, 0.3], [0.3, 0.5, 0.2], [0.2, 0.3, 0.5]])
    # B = np.array([[0.5, 0.2, 0.3], [0.4, 0.2, 0.4], [0.6, 0.3, 0.1]])
    # hmm_real = HMM(3, 3, S, A, B)
    # g = GenData(hmm_real, 500)
    # state, obs = g.gen_data()
    # æ£€æµ‹ç”Ÿæˆçš„æ•°æ®
    # state, obs = np.array(state), np.array(obs)
    # ind = np.where(state==2)[0]
    # from collections import Counter
    # obs_ind = obs[ind]
    # c1 = Counter(obs_ind)
    # n = sum(c1.values())
    # for o, val in c.items():
    #     print(o, val/n)
    # ind_next = ind + 1
    # ind_out = np.where(ind_next==1000)
    # ind_next = np.delete(ind_next, ind_out)
    # state_next = state[ind_next]
    # c2 = Counter(state_next)
    # n = sum(c2.values())
    # for s, val in c2.items():
    #     print(s, val/n)

    # é¢„æµ‹
    S = np.array([0.5, 0.5])
    A = np.array([[0.8, 1], [0.8, 0.8]])
    B = np.array([[0.2, 0.0, 0.8], [0, 0.8, 0.2]])
    hmm = HMM(2, 3, S, A, B)
    g = GenData(hmm, 200)
    state, obs = g.gen_data()
    print(obs)
    path = predict(hmm, obs)
    score = sum([int(i == j) for i, j in zip(state, path)])
    print(score / len(path))

    # å­¦ä¹ 
    # import matplotlib.pyplot as plt
    #
    #
    # def triangle_data(n_sample):
    #     # ç”Ÿæˆä¸‰è§’æ³¢å½¢çŠ¶çš„åºåˆ—
    #     data = []
    #     for x in range(n_sample):
    #         x = x % 6
    #         if x <= 3:
    #             data.append(x)
    #         else:
    #             data.append(6 - x)
    #     return data
    #
    #
    # hmm = HMM(10, 4)
    # data = triangle_data(30)
    # hmm = fit(hmm, data)
    # g = GenData(hmm, 30)
    # state, obs = g.gen_data()
    #
    # x = [i for i in range(30)]
    # plt.scatter(x, obs, marker='*', color='r')
    # plt.plot(x, data, color='g')
    # plt.show()
```

å‚è€ƒèµ„æ–™ï¼šhttps://www.zhihu.com/question/20962240ï¼ˆé©¬å°”å¯å¤«ï¼‰ã€https://blog.csdn.net/slx_share/article/details/80237566ï¼ˆéšé©¬å°”å¯å¤«ï¼‰ã€https://blog.csdn.net/slx_share/article/details/80237566ï¼ˆæºç æ¡ˆä¾‹ï¼‰

### ğŸ’¤ è’™ç‰¹å¡ç½—æ–¹æ³•ï¼ˆ**Monte Carlo** Methodï¼‰

#### âœ… åŸºç¡€æ¨¡å‹

**æ¦‚è¿°**ï¼šé€šè¿‡å¤§é‡éšæœºæ ·æœ¬ï¼Œå»äº†è§£ä¸€ä¸ªç³»ç»Ÿï¼Œè¿›è€Œå¾—åˆ°æ‰€è¦è®¡ç®—çš„å€¼ã€‚å®ƒéå¸¸å¼ºå¤§å’Œçµæ´»ï¼Œåˆç›¸å½“ç®€å•æ˜“æ‡‚ï¼Œå¾ˆå®¹æ˜“å®ç°ã€‚å¯¹äºè®¸å¤šé—®é¢˜æ¥è¯´ï¼Œå®ƒå¾€å¾€æ˜¯æœ€ç®€å•çš„è®¡ç®—æ–¹æ³•ï¼Œæœ‰æ—¶ç”šè‡³æ˜¯å”¯ä¸€å¯è¡Œçš„æ–¹æ³•ã€‚

**ä¸¾ä¾‹**ï¼šåœ¨åŒºåŸŸå†…éšæœºäº§ç”Ÿ10000ä¸ªç‚¹ï¼Œè½åœ¨åœ†å†…çš„ç‚¹ä¸åœ†å¤–çš„ç‚¹æ¯”ä¾‹è®¡åˆ’ï¼Œå¯ä»¥æ¨å¯¼ğ…çš„è¿‘ä¼¼å€¼

<img src="http://www.ruanyifeng.com/blogimg/asset/2015/bg2015072604.jpg" style="width:30%"/>

```python
import random
def calpai():
    n = 1000000
    r = 1.0
    a, b = (0.0, 0.0)
    x_neg, x_pos = a - r, a + r
    y_neg, y_pos = b - r, b + r

    count = 0
    for i in range(0, n):
        x = random.uniform(x_neg, x_pos)
        y = random.uniform(y_neg, y_pos)
        if x*x + y*y <= 1.0:
            count += 1

    print (count / float(n)) * 4
```

#### ğŸ†˜ Inverse CDF æ–¹æ³•

**åŸç†**ï¼šç»å…¸ä¸”å¸¸è§çš„æ¨¡å‹å¦‚æŒ‡æ•°åˆ†å¸ƒã€ğ›¾ åˆ†å¸ƒã€t åˆ†å¸ƒã€F åˆ†å¸ƒã€Î² åˆ†å¸ƒã€Dirichlet åˆ†å¸ƒéƒ½æ˜¯æœ‰çš„ï¼Œå¯ä»¥æ–¹ä¾¿é‡‡æ ·ï¼Œä½†æ˜¯å¯¹äºç›¸å¯¹å¤æ‚çš„åˆ†å¸ƒï¼Œå°±éœ€è¦è®¾è®¡é‡‡æ ·ç­–ç•¥ï¼Œæ¯”å¦‚Inverse CDFï¼ˆCumulative Distribution Functionï¼‰æ–¹æ³•ï¼ŒCDFå¯ä»¥ç”±æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ˆPDFï¼ŒProbability Density Functionï¼‰è¿›è¡Œç§¯åˆ†å¾—åˆ°

editing.....

#### ğŸ†˜ æ‹’ç»æ¥å—é‡‡æ ·

**æ¡ˆä¾‹**ï¼šå‡è®¾ä½¿ç”¨ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=U%280%2C1%29) æ¥ä½œä¸ºâ€œproposal distributionâ€ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=G) ï¼Œè¿™æ · ![[å…¬å¼]](https://www.zhihu.com/equation?tex=g%28x%29%3D1%5Cforall+x%5Cin+%5B0%2C1%5D) ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬æ¯æ¬¡ç”Ÿæˆçš„ä¸¤ä¸ªæ ·æœ¬ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=Y) ä¸ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=U) ï¼Œå¯¹åº”ä¸‹å›¾ä¸­çŸ©å½¢å†…çš„ä¸€ç‚¹ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=P%28Y%2CU%E2%88%97c%E2%88%97g%28Y%29%29) ã€‚æ¥å—æ¡ä»¶ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=U%5Cleqslant+f%28Y%29c%E2%88%97g%28Y%29) ï¼Œå³ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=U%E2%88%97c%E2%88%97g%28Y%29%5Cleqslant+f%28Y%29) çš„å‡ ä½•æ„ä¹‰æ˜¯ç‚¹ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=P) åœ¨ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=f%28x%29) ä¸‹æ–¹ï¼Œä¸æ¥å— ![[å…¬å¼]](https://www.zhihu.com/equation?tex=Y) çš„å‡ ä½•æ„ä¹‰æ˜¯ç‚¹ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=P) åœ¨ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=f%28x%29) çš„ä¸Šæ–¹ã€‚åœ¨ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=f%28x%29) ä¸‹æ–¹çš„ç‚¹(oå½¢çŠ¶)æ»¡è¶³æ¥å—æ¡ä»¶ï¼Œä¸Šæ–¹çš„ç‚¹(+å½¢çŠ¶)ä¸æ»¡è¶³æ¥å—æ¡ä»¶ã€‚ 

<img src="https://pic1.zhimg.com/80/v2-ba61bc580f87754f4e35675c6ac1b23c_hd.jpg" style="width:50%"/>

**ä»£ç å®ç°**ï¼šå‡å¦‚æˆ‘ä»¬çš„ç›®æ ‡æ¦‚ç‡å¯†åº¦å‡½æ•°æ˜¯ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=f%28x%29%3D%5Cfrac%7B%28x-0.4%29%5E%7B4%7D%7D%7B%5Cint_%7B0%7D%5E%7B1%7D%28x-0.4%29%5E%7B4%7Ddx%7D) ï¼Œå¯¹æ­¤åˆ†å¸ƒç”Ÿæˆæ ·æœ¬ã€‚

```python
import random
import math
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

%matplotlib inline
sns.set_style('darkgrid')
plt.rcParams['figure.figsize'] = (12, 8)

def AceeptReject(split_val):
    global c
    global power
    while True:
        x = random.uniform(0, 1)
        y = random.uniform(0, 1)
        if y*c <= math.pow(x - split_val, power):
            return x

power = 4
t = 0.4  
sum_ = (math.pow(1-t, power + 1) - math.pow(-t, power + 1)) / (power + 1)  #æ±‚ç§¯åˆ†
x = np.linspace(0, 1, 100)
#å¸¸æ•°å€¼c
c = 0.6**4/sum_
cc = [c for xi in x]
plt.plot(x, cc, '--',label='c*f(x)')
#ç›®æ ‡æ¦‚ç‡å¯†åº¦å‡½æ•°çš„å€¼f(x)
y = [math.pow(xi - t, power)/sum_ for xi in x]
plt.plot(x, y,label='f(x)')
#é‡‡æ ·10000ä¸ªç‚¹
samples = []
for  i in range(10000):
    samples.append(AceeptReject(t))
plt.hist(samples, bins=50, normed=True,label='sampling')
plt.legend()
plt.show()
```

<img src="https://pic2.zhimg.com/80/v2-8ca3019c1c1b0a878f44f51458c5b15d_hd.jpg" alt="img" style="zoom:70%;" />

#### ğŸ†˜ è‡ªé€‚åº”çš„æ‹’ç»é‡‡æ ·ï¼ˆAdaptive Rejection Samplingï¼‰

editing.....

å‚è€ƒèµ„æ–™ï¼šhttp://www.ruanyifeng.com/blog/2015/07/monte-carlo-method.html è’™ç‰¹å¡æ´›ã€https://blog.csdn.net/baimafujinji/article/details/51407703 è’™ç‰¹å¡æ´›é‡‡æ ·ä¹‹æ‹’ç»é‡‡æ ·ï¼ˆReject Samplingï¼‰

### ğŸ’¤ é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡æ´›ï¼ˆMarkov chain Monte Carloï¼‰

#### âœ… èƒŒæ™¯

**åŠ¨æœºä¸€**

å‡å¦‚ä½ éœ€è¦å¯¹ä¸€ç»´éšæœºå˜é‡$X$è¿›è¡Œé‡‡æ ·ï¼Œ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=X) çš„æ ·æœ¬ç©ºé—´æ˜¯ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=%5C%7B1%2C2%2C3%5C%7D) ï¼Œä¸”æ¦‚ç‡åˆ†åˆ«æ˜¯ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=%5C%7B1%2F2%2C1%2F4%2C1%2F4%5C%7D) ï¼Œè¿™å¾ˆç®€å•ï¼Œåªéœ€å†™è¿™æ ·ç®€å•çš„ç¨‹åºï¼šé¦–å…ˆæ ¹æ®å„ç¦»æ•£å–å€¼çš„æ¦‚ç‡å¤§å°å¯¹ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=%5B0%2C1%5D)åŒºé—´è¿›è¡Œç­‰æ¯”ä¾‹åˆ’åˆ†ï¼Œå¦‚åˆ’åˆ†ä¸º ![[å…¬å¼]](https://www.zhihu.com/equation?tex=%5B0%2C0.5%5D%2C%5B0%2C5%2C0.75%5D%2C%5B0.75%2C1%5D) è¿™ä¸‰ä¸ªåŒºé—´ï¼Œå†é€šè¿‡è®¡ç®—æœºäº§ç”Ÿ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=%5B0%2C1%5D) ä¹‹é—´çš„ä¼ªéšæœºæ•°ï¼Œæ ¹æ®ä¼ªéšæœºæ•°çš„è½ç‚¹å³å¯å®Œæˆä¸€æ¬¡é‡‡æ ·ã€‚æ¥ä¸‹æ¥ï¼Œå‡å¦‚ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=X) æ˜¯è¿ç»­åˆ†å¸ƒçš„å‘¢ï¼Œæ¦‚ç‡å¯†åº¦æ˜¯ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=f%28X%29) ï¼Œé‚£è¯¥å¦‚ä½•è¿›è¡Œé‡‡æ ·å‘¢ï¼Ÿèªæ˜çš„ä½ è‚¯å®šä¼šæƒ³åˆ°ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ï¼Œ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=P%28X%3Ct%29%3D%5Cint+_%7B-%5Cinfty%7D%5E%7Bt%7Df%28x%29dx) ï¼Œå³åœ¨ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=%5B0%2C1%5D) é—´éšæœºç”Ÿæˆä¸€ä¸ªæ•° ![[å…¬å¼]](https://www.zhihu.com/equation?tex=a) ï¼Œç„¶åæ±‚ä½¿å¾—ä½¿ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=P%28x%3Ct%29%3Da) æˆç«‹çš„ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=t) ï¼Œ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=t) å³å¯ä»¥è§†ä½œä»è¯¥åˆ†éƒ¨ä¸­å¾—åˆ°çš„ä¸€ä¸ªé‡‡æ ·ç»“æœã€‚è¿™é‡Œæœ‰ä¸¤ä¸ªå‰æï¼šä¸€æ˜¯æ¦‚ç‡å¯†åº¦å‡½æ•°å¯ç§¯ï¼›ç¬¬äºŒä¸ªæ˜¯ç´¯ç§¯åˆ†å¸ƒå‡½æ•°æœ‰åå‡½æ•°ã€‚å‡å¦‚æ¡ä»¶ä¸æˆç«‹æ€ä¹ˆåŠå‘¢ï¼ŸMCMCå°±ç™»åœºäº†ã€‚

**åŠ¨æœºäºŒ**

å‡å¦‚å¯¹äºé«˜ç»´éšæœºå˜é‡ï¼Œæ¯”å¦‚ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=%5Cmathbb%7BR%7D%5E%7B50%7D) ï¼Œè‹¥æ¯ä¸€ç»´å–100ä¸ªç‚¹ï¼Œåˆ™æ€»å…±è¦å– ![[å…¬å¼]](https://www.zhihu.com/equation?tex=10%5E%7B100%7D) ï¼Œè€Œå·²çŸ¥å®‡å®™çš„åŸºæœ¬ç²’å­å¤§çº¦æœ‰ ![[å…¬å¼]](https://www.zhihu.com/equation?tex=10%5E%7B87%7D) ä¸ªï¼Œå¯¹è¿ç»­çš„ä¹ŸåŒæ ·å¦‚æ­¤ã€‚å› æ­¤MCMCå¯ä»¥è§£å†³â€œç»´æ•°ç¾éš¾â€é—®é¢˜ã€‚

#### ğŸ†˜ M-Hé‡‡æ ·



#### ğŸ†˜ Gibbsé‡‡æ ·



å‚è€ƒèµ„æ–™ï¼šhttps://zhuanlan.zhihu.com/p/37121528 MCMCâ€”â€”å…¶å®æ²¡å¤ªçœ‹æ‡‚
